{
    "docs": [
        {
            "location": "/", 
            "text": "Monstache\n\n\nSync MongoDB to Elasticsearch in realtime\n\n\n\n\nMonstache\n is a sync daemon written in Go that continously \nindexes your MongoDB collections into Elasticsearch. Monstache gives you the ability to use \nElasticsearch to do complex searches and aggregations of your MongoDB data and easily build realtime \nKibana visualizations and dashboards.\n\n\nFeatures\n\n\n\n\n\n\nSupports up to and including the latest versions of Elasticsearch and MongoDB\n\n\n\n\n\n\nSingle binary with a light footprint \n\n\n\n\n\n\nPre built Docker \ncontainers\n\n\n\n\n\n\nOptionally filter the set of collections to sync\n\n\n\n\n\n\nAdvanced support for sharded MongoDB clusters including auto-detection of new shards\n\n\n\n\n\n\nDirect read mode to do a full sync of collections in addition to tailing the oplog\n\n\n\n\n\n\nTransform and filter documents before indexing using Golang plugins or JavaScript\n\n\n\n\n\n\nIndex the content of GridFS files\n\n\n\n\n\n\nSupport for hard and soft deletes in MongoDB\n\n\n\n\n\n\nSupport for propogating database and collection drops\n\n\n\n\n\n\nOptional custom document routing in Elasticsearch\n\n\n\n\n\n\nStateful resume feature\n\n\n\n\n\n\nTime machine feature to track document changes over time\n\n\n\n\n\n\nWorker and Clustering modes for High Availability\n\n\n\n\n\n\nSupport for \nrfc7396\n JSON merge patches\n\n\n\n\n\n\nSystemd support\n\n\n\n\n\n\nOptional http server to get access to liveness, stats, etc\n\n\n\n\n\n\nSee \nGetting Started\n for instructions how to get\nit up and running.\n\n\nSee the \nRelease Notes\n for updates.", 
            "title": "Home"
        }, 
        {
            "location": "/#monstache", 
            "text": "Sync MongoDB to Elasticsearch in realtime   Monstache  is a sync daemon written in Go that continously \nindexes your MongoDB collections into Elasticsearch. Monstache gives you the ability to use \nElasticsearch to do complex searches and aggregations of your MongoDB data and easily build realtime \nKibana visualizations and dashboards.", 
            "title": "Monstache"
        }, 
        {
            "location": "/#features", 
            "text": "Supports up to and including the latest versions of Elasticsearch and MongoDB    Single binary with a light footprint     Pre built Docker  containers    Optionally filter the set of collections to sync    Advanced support for sharded MongoDB clusters including auto-detection of new shards    Direct read mode to do a full sync of collections in addition to tailing the oplog    Transform and filter documents before indexing using Golang plugins or JavaScript    Index the content of GridFS files    Support for hard and soft deletes in MongoDB    Support for propogating database and collection drops    Optional custom document routing in Elasticsearch    Stateful resume feature    Time machine feature to track document changes over time    Worker and Clustering modes for High Availability    Support for  rfc7396  JSON merge patches    Systemd support    Optional http server to get access to liveness, stats, etc    See  Getting Started  for instructions how to get\nit up and running.  See the  Release Notes  for updates.", 
            "title": "Features"
        }, 
        {
            "location": "/start/", 
            "text": "Getting Started\n\n\n\n\nInstallation\n\n\nMonstache is just a single binary without dependencies on runtimes like Ruby, Python or PHP.\n\n\nYou just need to \ndownload the latest version\n.\n\n\nYou will want to use 4.x releases for ES6+ and 3.x releases for ES2-5.\n\n\nUnzip the download and adjust your PATH variable to include the path to the folder for your platform.\n\n\nLet's make sure Monstache is set up as expected. You should see a similar version number in your terminal:\n\n\nmonstache -v\n# 4.6.4\n\n\n\n\nThe version number should start with 3.x if you are using Elasticsearch prior to version 6.\n\n\n\n\nYou can also build monstache from source. For Elasticsearch 6 and up use\n\n\ngo get -u github.com/rwynn/monstache\n\n\nFor Elasticsearch before version 6 use\n\n\ngo get -u gopkg.in/rwynn/monstache.v3\n\n\n\n\nUsage\n\n\nMonstache uses the MongoDB \noplog\n as an event source. You will need to make sure that MongoDB is configured to\nproduce an oplog.  The oplog can be enabled by using one of the following options:\n\n\n\n\nSetting up \nreplica sets\n\n\nPassing --master to the mongod process\n\n\nSetting the following in /etc/mongod.conf\n\n\n\n\nmaster = true\n\n\n\n\n\n\nNote\n\n\nIf you have enabled security in MongoDB you will need to give the user in your connection string\ncertain privileges.  Specifically, the user will need to be able read the \nlocal\n database (to read\nfrom the oplog) and any user databases that you wish to synch data from.  Additionally, when using the \nresume or clustering features the user will need to be able to write to and create indexes for the \n\nmonstache\n database. \n\n\n\n\nMonstache makes concurrent bulk indexing requests to Elasticsearch.  It is recommended to increase the\npool of bulk request handlers configured for Elasticsearch to ensure that requests do not begin to time\nout due to an overloaded queue. The queue size can be increased by making changes to your elasticsearch.yml\nconfiguration.\n\n\n thread_pool:\n   bulk:\n     queue_size: 200\n\n\n\n\nWithout any explicit configuration monstache will connect to Elasticsearch and MongoDB on localhost\non the default ports and begin tailing the MongoDB oplog.  Any changes to MongoDB will be reflected in Elasticsearch.\n\n\nMonstache uses the \nTOML\n format for its configuration.  You can run \nmonstache with an explicit configuration by passing the -f flag.\n\n\nmonstache -f /path/to/config.toml\n\n\n\n\nThe following shows how to specify options in a TOML config file.  It is recommended that you start with only your MongoDB\nand Elasticsearch connection settings and only specify additional options as needed. \n\n\n# connection settings\n\n# connect to MongoDB using the following URL\nmongo-url = \nmongodb://someuser:password@localhost:40001\n\n# connect to the Elasticsearch REST API at the following URLs\nelasticsearch-urls = [\nhttps://example:9200\n]\n\n\n# additional settings\n\n# compress requests to Elasticsearch\ngzip = true\n# generate indexing statistics\nstats = true\n# index statistics into Elasticsearch\nindex-stats = true\n# use the following PEM file for connections to MongoDB\nmongo-pem-file = \n/path/to/mongoCert.pem\n\n# disable PEM validation\nmongo-validate-pem-file = false\n# use the following user name for Elasticsearch basic auth\nelasticsearch-user = \nsomeuser\n\n# use the following password for Elasticsearch basic auth\nelasticsearch-password = \nsomepassword\n\n# use 10 go routines concurrently pushing documents to Elasticsearch\nelasticsearch-max-conns = 10\n# use the following PEM file to connections to Elasticsearch\nelasticsearch-pem-file = \n/path/to/elasticCert.pem\n\n# validate connections to Elasticsearch\nelastic-validate-pem-file = true\n# propogate dropped collections in MongoDB as index deletes in Elasticsearch\ndropped-collections = true\n# propogate dropped databases in MongoDB as index deletes in Elasticsearch\ndropped-databases = true\n# do not start processing at the beginning of the MongoDB oplog\nreplay = false\n# resume processing from a timestamp saved in a previous run\nresume = true\n# do not validate that progress timestamps have been saved\nresume-write-unsafe = false\n# override the name under which resume state is saved\nresume-name = \ndefault\n\n# include documents whose namespace matches the following pattern\nnamespace-regex = '^mydb\\.(mycollection|\\$cmd)$'\n# exclude documents whose namespace matches the following pattern\nnamespace-exclude-regex = '^mydb\\.(ignorecollection|\\$cmd)$'\n# turn on indexing of GridFS file content\nindex-files = true\n# turn on search result highlighting of GridFS content\nfile-highlighting = true\n# index GridFS files inserted into the following collections\nfile-namespaces = [\nusers.fs.files\n]\n# print detailed information including request traces\nverbose = true\n# enable clustering mode\ncluster-name = 'apollo'\n# do a full-sync of the following collections\ndirect-read-namespaces = [\ndb.collection\n, \ntest.test\n]\n# do not exit after full-sync, rather continue tailing the oplog\nexit-after-direct-reads = false\n\n\n\n\nSee \nConfiguration\n for details about each configuration\noption.", 
            "title": "Getting Started"
        }, 
        {
            "location": "/start/#getting-started", 
            "text": "", 
            "title": "Getting Started"
        }, 
        {
            "location": "/start/#installation", 
            "text": "Monstache is just a single binary without dependencies on runtimes like Ruby, Python or PHP.  You just need to  download the latest version .  You will want to use 4.x releases for ES6+ and 3.x releases for ES2-5.  Unzip the download and adjust your PATH variable to include the path to the folder for your platform.  Let's make sure Monstache is set up as expected. You should see a similar version number in your terminal:  monstache -v\n# 4.6.4  The version number should start with 3.x if you are using Elasticsearch prior to version 6.   You can also build monstache from source. For Elasticsearch 6 and up use  go get -u github.com/rwynn/monstache  For Elasticsearch before version 6 use  go get -u gopkg.in/rwynn/monstache.v3", 
            "title": "Installation"
        }, 
        {
            "location": "/start/#usage", 
            "text": "Monstache uses the MongoDB  oplog  as an event source. You will need to make sure that MongoDB is configured to\nproduce an oplog.  The oplog can be enabled by using one of the following options:   Setting up  replica sets  Passing --master to the mongod process  Setting the following in /etc/mongod.conf   master = true   Note  If you have enabled security in MongoDB you will need to give the user in your connection string\ncertain privileges.  Specifically, the user will need to be able read the  local  database (to read\nfrom the oplog) and any user databases that you wish to synch data from.  Additionally, when using the \nresume or clustering features the user will need to be able to write to and create indexes for the  monstache  database.    Monstache makes concurrent bulk indexing requests to Elasticsearch.  It is recommended to increase the\npool of bulk request handlers configured for Elasticsearch to ensure that requests do not begin to time\nout due to an overloaded queue. The queue size can be increased by making changes to your elasticsearch.yml\nconfiguration.   thread_pool:\n   bulk:\n     queue_size: 200  Without any explicit configuration monstache will connect to Elasticsearch and MongoDB on localhost\non the default ports and begin tailing the MongoDB oplog.  Any changes to MongoDB will be reflected in Elasticsearch.  Monstache uses the  TOML  format for its configuration.  You can run \nmonstache with an explicit configuration by passing the -f flag.  monstache -f /path/to/config.toml  The following shows how to specify options in a TOML config file.  It is recommended that you start with only your MongoDB\nand Elasticsearch connection settings and only specify additional options as needed.   # connection settings\n\n# connect to MongoDB using the following URL\nmongo-url =  mongodb://someuser:password@localhost:40001 \n# connect to the Elasticsearch REST API at the following URLs\nelasticsearch-urls = [ https://example:9200 ]\n\n\n# additional settings\n\n# compress requests to Elasticsearch\ngzip = true\n# generate indexing statistics\nstats = true\n# index statistics into Elasticsearch\nindex-stats = true\n# use the following PEM file for connections to MongoDB\nmongo-pem-file =  /path/to/mongoCert.pem \n# disable PEM validation\nmongo-validate-pem-file = false\n# use the following user name for Elasticsearch basic auth\nelasticsearch-user =  someuser \n# use the following password for Elasticsearch basic auth\nelasticsearch-password =  somepassword \n# use 10 go routines concurrently pushing documents to Elasticsearch\nelasticsearch-max-conns = 10\n# use the following PEM file to connections to Elasticsearch\nelasticsearch-pem-file =  /path/to/elasticCert.pem \n# validate connections to Elasticsearch\nelastic-validate-pem-file = true\n# propogate dropped collections in MongoDB as index deletes in Elasticsearch\ndropped-collections = true\n# propogate dropped databases in MongoDB as index deletes in Elasticsearch\ndropped-databases = true\n# do not start processing at the beginning of the MongoDB oplog\nreplay = false\n# resume processing from a timestamp saved in a previous run\nresume = true\n# do not validate that progress timestamps have been saved\nresume-write-unsafe = false\n# override the name under which resume state is saved\nresume-name =  default \n# include documents whose namespace matches the following pattern\nnamespace-regex = '^mydb\\.(mycollection|\\$cmd)$'\n# exclude documents whose namespace matches the following pattern\nnamespace-exclude-regex = '^mydb\\.(ignorecollection|\\$cmd)$'\n# turn on indexing of GridFS file content\nindex-files = true\n# turn on search result highlighting of GridFS content\nfile-highlighting = true\n# index GridFS files inserted into the following collections\nfile-namespaces = [ users.fs.files ]\n# print detailed information including request traces\nverbose = true\n# enable clustering mode\ncluster-name = 'apollo'\n# do a full-sync of the following collections\ndirect-read-namespaces = [ db.collection ,  test.test ]\n# do not exit after full-sync, rather continue tailing the oplog\nexit-after-direct-reads = false  See  Configuration  for details about each configuration\noption.", 
            "title": "Usage"
        }, 
        {
            "location": "/config/", 
            "text": "Configuration\n\n\n\n\nConfiguration can be specified in your TOML config file or be passed into monstache as Go program arguments on the command line.\nProgram arguments take precedance over configuration specified in the TOML config file.\n\n\n\n\nWarning\n\n\nPlease keep any simple -one line config- above any \n[[script]]\n or toml table configs, as \nthere is a bug\n in the toml parser where if you have definitions below a TOML table, e.g. a \n[[script]]\n then the parser thinks that lines below that belong to the table instead of at the global level\n\n\n\n\nprint-config\n\n\nboolean (default false)\n\n\nWhen print-config is true monstache will print its configuration and then exit\n\n\nstats\n\n\nboolean (default false)\n\n\nWhen stats is true monstache will periodically print statistics accumulated by the indexer\n\n\nstats-duration\n\n\nstring (default 30s)\n\n\nSets the duration after which statistics are printed if stats is enabled\n\n\nindex-stats\n\n\nboolean (default false)\n\n\nWhen index-stats is true monstache will write statistics about its indexing progress in\nElasticsearch.  The indexes used to store the statistics are time stamped by day and \nprefixed \nmonstache.stats.\n. E.g. monstache.stats.2017-07-01 and so on. \n\n\nAs these indexes will accrue over time your can use a tool like \ncurator\n\nto prune them with a Delete Indices action and an age filter.\n\n\nstats-index-format\n\n\nstring (default monstache.stats.2006-01-02)\n\n\nThe \ntime.Time\n supported index name format for stats indices.  By default, stats indexes \nare partitioned by day.  To use less indices for stats you can shorten this format string \n(e.g monstache.stats.2006-01) or remove the time component completely to use a single index.  \n\n\ngzip\n\n\nboolean (default false)\n\n\nWhen gzip is true, monstache will compress requests to elasticsearch to increase performance. \nIf you enable gzip in monstache and are using elasticsearch prior to version 5 you will also \nneed to update the elasticsearch config file to set http.compression: true. In elasticsearch \nversion 5 and above http.compression is enabled by default. Enabling gzip is recommended \nespecially if you enable the index-files setting.\n\n\nfail-fast\n\n\nboolean (default false)\n\n\nWhen fail-fast is true, if monstache receives a failed bulk indexing response from Elasticsearch, monstache\nwill log the request that produced the response as an ERROR and then exit immediately with an error status. \nNormally, monstache just logs the error and continues processing events.\n\n\nIf monstache has been configured with \nelasticsearch-retry\n true, a failed request will be\nretried before being considered a failure.\n\n\nprune-invalid-json\n\n\nboolean (default false)\n\n\nIf you MongoDB data contains values like +Infinity, -Infinity, NaN, or invalid dates you will want to set this option to true.  The\nGolang json serializer is not able to handle these values and the indexer will get stuck in an infinite loop. When this\nis set to true Monstache will drop those fields so that indexing errors do not occur.\n\n\nindex-oplog-time\n\n\nboolean (default false)\n\n\nIf this option is set to true monstache will include 2 automatic fields in the source document indexed into\nElasticsearch.  The first is _oplog_ts which is the timestamp for the event copied directly from the MongoDB\noplog. The second is _oplog_date which is an Elasticsearch date field corresponding to the time of the same\nevent.\n\n\nThis information is generally useful in Elasticsearch giving the notion of last updated.  However, it's also\nvaluable information to have for failed indexing requests since it gives one the information to replay from \na failure point.  See the option \nresume-from-timestamp\n for information on how to replay oplog events since \na given event occurred. \n\n\nFor data read via the direct read feature the oplog time will only be available if the id of the MongoDB\ndocument is an ObjectID.  If the id of the MongoDB document is not an ObjectID and the document source is\na direct read query then the oplog time with not be available.\n\n\nresume\n\n\nboolean (default false)\n\n\nWhen resume is true, monstache writes the timestamp of mongodb operations it has successfully synced to elasticsearch\nto the collection monstache.monstache.  It also reads the timestamp from that collection when it starts in order to replay\nevents which it might have missed because monstache was stopped. If monstache is started with the \ncluster-name\n option\nset then resume is automatically turned on.  \n\n\nresume-name\n\n\nstring (default \"default\")\n\n\nmonstache uses the value of resume-name as an id when storing and retrieving timestamps\nto and from the mongodb collection monstache.monstache. The default value for this option is \ndefault\n.\nHowever, there are some exceptions.  If monstache is started with the \ncluster-name\n option set then the\nname of the cluster becomes the resume-name.  This is to ensure that any process in the cluster is able to resume\nfrom the last timestamp successfully processed.  The other exception occurs when resume-name is not given but\n\nworker-name\n is.  In that cause the worker name becomes the resume-name.\n\n\nresume-from-timestamp\n\n\nint64 (default 0)\n\n\nWhen resume-from-timestamp (a 64 bit timestamp where the high 32 bytes represent the number of seconds since epoch and the low 32 bits\nrepresent an offset within a second) is given, monstache will sync events starting immediately after the timestamp.  This is useful if you have \na specific timestamp from the oplog and would like to start syncing from after this event. \n\n\nreplay\n\n\nboolean (default false)\n\n\nWhen replay is true, monstache replays all events from the beginning of the mongodb oplog and syncs them to elasticsearch.\n\n\nWhen \nresume\n and replay are both true, monstache replays all events from the beginning of the mongodb oplog, syncs them\nto elasticsearch and also writes the timestamp of processed events to monstache.monstache. \n\n\nWhen neither resume nor replay are true, monstache reads the last timestamp in the oplog and starts listening for events\noccurring after this timestamp.  Timestamps are not written to monstache.monstache.  This is the default behavior. \n\n\nresume-write-unsafe\n\n\nboolean (default false)\n\n\nWhen resume-write-unsafe is true monstache sets the safety mode of the mongodb session such that writes are fire and forget.\nThis speeds up writing of timestamps used to resume synching in a subsequent run of monstache.  This speed up comes at the cost\nof no error checking on the write of the timestamp.  Since errors writing the last synched timestamp are only logged by monstache\nand do not stop execution it's not unreasonable to set this to true to get a speedup.\n\n\ntime-machine-namespaces\n\n\n[]string (default nil)\n\n\nMonstache is good at keeping your MongoDB collections and Elasticsearch indexes in sync.  When a document is updated in MongoDB the corresponding document in Elasticsearch is updated too.  Same goes for deleting documents in MongoDB.  But what if you also wanted to keep a log of all the changes to a MongoDB document over its lifespan.  That's what time-machine-namespaces are for.  When you configure a list of namespaces in MongoDB to add to the time machine, in addition to keeping documents in sync, Monstache will index of copy of your MongoDB document at the time it changes in a separate timestamped index.\n\n\nSay for example, you insert a document into the \ntest.test\n collection in MongoDB.  Monstache will index by default into the \ntest.test\n index in Elasticsearch, but with time machines it will also index it into \nlog.test.test.2018-02-19\n.  When it indexes it into the time machine index it does so without the id from MongoDB and lets Elasticsearch generate a unique id.  But, it stores the id from MongoDB in the source field \n_source_id\n.  Also, it adds _oplog_ts and _oplog_date fields on the source document.  These correspond to the timestamp from the oplog when the data changed in MongoDB. Finally, it routes the document by the MongoDB id so that you can speed up queries later to find changes to a doc.\n\n\nThis lets you do some cool things but mostly you'll want to sort by \n_oplog_date\n and filter by \n_source_id\n to see how documents have changed over time. \n\n\nBecause the indexes are timestamped you can drop then after a period of time so they don't take up space.  If you just want the last couple of days of changes, delete the indexes with the old timestamps.  Elastic \ncurator\n is your friend here.\n\n\nThis option may be passed on the command line as ./monstache --time-machine-namespace test.foo --time-machine-namespace test.bar\n\n\ntime-machine-index-prefix\n\n\nstring (default \"log\")\n\n\nIf you have enabled time machine namespaces and want to change the prefix assigned to the index names use this setting.\n\n\ntime-machine-index-suffix\n\n\nstring (default \"2006-01-02\")\n\n\nIf you have enabled time machine namespaces and want to suffix the index names using a different date format use this setting.  Consult the golang docs for how date formats work.  By default this suffixes the index name with the year, month, and day.\n\n\ntime-machine-direct-reads\n\n\nboolean (default false)\n\n\nThis setting controls whether or not direct reads are added to the time machine log index. This is false by default so only changes read from the oplog are added. \n\n\nrouting-namespaces\n\n\n[]string (default nil)\n\n\nYou only need to set this configuration option if you use golang and javascript plugins are do custom routing: override parent or routing attributes. This array should be set to a list of all the namespaces that custom routing is done on. This ensures that deletes in MongoDB are routed correctly to \nElasticsearch.\n\n\ndelete-strategy\n\n\nint (default 0)\n\n\nThe strategy to use for handling document deletes when custom indexing is done in scripts.\n\n\n\n\nWarning\n\n\nBreaking change in versions \nv4.4.0\n \n \nv3.11.0\n. Monstache was saving routing foreach document in mongodb in a db called \nmonstache\n collection \nmeta\n using the same mongodb URL and credentials provided in config by default.\nMonstache was only saving this information if the document metadata was being altered via \n_monstache_meta\n in a script or via the API in a golang plugin.  Monstache needed to save this information in order to locate and perform deletes in Elasticsearch if the corresponding document was deleted in MongoDB.\nIf you want to maintain this stategy use value 1.  Otherwise, you can drop the \nmonstache.meta\n collection as this is no longer used by default.\n\n\n\n\nBut now this has changed to be stateless, you can read more: \ndiscussion\n \n \ncommit\n\n\nStrategy 0\n -default- will do a term query by document id across all Elasticsearch indexes. Will only perform the delete if one single document is returned by the query.\n\n\nStategy 1\n will store indexing metadata in MongoDB in the \nmonstache.meta\n collection and use this metadata to locate and delete the document.\n\n\nStategy 2\n will completely ignore document deletes in MongoDB.\n\n\ndelete-index-pattern\n\n\nstring (default *)\n\n\nWhen using a stateless delete strategy, set this to a valid Elasticsearch index pattern to restrict the scope of possible indexes that a stateless delete\nwill consider.  If monstache only indexes to index a, b, and c then you can set this to \na,b,c\n.  If monstache only indexes to indexes starting with \nmydb then you can set this to \nmydb*\n.  \n\n\ndirect-read-namespaces\n\n\n[]string (default nil)\n\n\nAt times even being able to replay from the beginning of the oplog is not enough to sync all of your mongodb data.\nThe oplog is a capped collection and may only contain a subset of the data.  In this case you can perform a direct\nsync of mongodb to elasticsearch.  To do this, set direct-read-namespaces to an array of namespaces that you would \nlike to copy.  Monstache will perform reads directly from the given set of db.collection and sync them to elasticsearch.\n\n\nThis option may be passed on the command line as ./monstache --direct-read-namespace test.foo --direct-read-namespace test.bar\n\n\nBy default, Monstache maps a MongoDB collection named \nfoo\n in a database named \ntest\n to the \ntest.foo\n index in Elasticsearch.\n\n\nFor maximum indexing performance when doing alot of a direct reads you will want to adjust the refresh interval during indexing on the\ndestination Elasticsearch indices.  The refresh interval can be set at a global level in elasticsearch.yml or on a per\nindex basis by using the Index Settings or Index Template APIs.  For more information see \nUpdate Indices Settings\n.\n\n\nBy default, Elasticsearch refreshes every second.  You will want to increase this value or turn off refresh completely during the indexing\nphase by setting the refresh_interval to -1.  Remember to reset the refresh_interval to a positive value and do a force merge after the indexing \nphase has completed if you decide to temporarily turn off refresh, otherwise you will not be able to see the new documents in queries.\n\n\ndirect-read-split-max\n\n\nint (default 9)\n\n\nThe maximum number of times to split a collection for direct reads.  This setting greatly impacts the memory consumption\nof Monstache.  When direct reads are performed, the collection is first broken up into ranges which are then read \nconcurrently is separate go routines.  If you increase this value you will notice the connection count increase in mongostat\nwhen direct reads are performed.  You will also notice the memory consumption of Monstache grow.  Increasing this value can\nincrease the throughput for reading large collections, but you need to have enough memory available to Monstache to do so.\nYou can decrease this value for a memory constrained Monstache process.\n\n\nexit-after-direct-reads\n\n\nboolean (default false)\n\n\nThe \ndirect-read-namespaces\n option gives you a way to do a full sync on multiple collections.  At times you may want\nto perform a full sync via the direct-read-namespaces option and then quit monstache.  Set this option to true and\nmonstache will exit after syncing the direct read collections instead of continuing to tail the oplog. This is useful\nif you would like to run monstache to run a full sync on a set of collections via a cron job.\n\n\nnamespace-regex\n\n\nregexp (default \"\")\n\n\nWhen namespace-regex is given this regex is tested against the namespace, database.collection, of the event. If\nthe regex matches monstache continues processing event filters, otherwise it drops the event. By default monstache\nprocesses events in all databases and all collections with the exception of the reserved database monstache, any\ncollections suffixed with .chunks, and the system collections. For more information see the section \nNamespaces\n.\n\n\nnamespace-exclude-regex\n\n\nregex (default \"\")\n\n\nWhen namespace-exclude-regex is given this regex is tested against the namespace, database.collection, of the event. If\nthe regex matches monstache ignores the event, otherwise it continues processing event filters. By default monstache\nprocesses events in all databases and all collections with the exception of the reserved database monstache, any\ncollections suffixed with .chunks, and the system collections. For more information see the section \nNamespaces\n.\n\n\nmongo-url\n\n\nstring (default localhost)\n\n\nThe URL to connect to MongoDB which must follow the \nStandard Connection String Format\n\n\nFor sharded clusters this URL should point to the \nmongos\n router server and the \nmongo-config-url\n\noption should be set to point to the config server.\n\n\nmongo-config-url\n\n\nstring (default \"\")\n\n\nThis config should only be set for sharded MongoDB clusters. Has the same syntax as mongo-url.\nThis URL should point to the MongoDB \nconfig\n server.\n\n\nMonstache will read the list of shards using this connection and then setup a listener to react\nto new shards being added to the cluster at a later time.\n\n\nmongo-pem-file\n\n\nstring (default \"\")\n\n\nWhen mongo-pem-file is given monstache will use the given file path to add a local certificate to x509 cert\npool when connecting to mongodb. This should only be used when mongodb is configured with SSL enabled.\n\n\nmongo-validate-pem\n\n\nboolean (default true)\n\n\nWhen mongo-validate-pem-file is false TLS will be configured to skip verification\n\n\nmongo-oplog-database-name\n\n\nstring (default local)\n\n\nWhen mongo-oplog-database-name is given monstache will look for the mongodb oplog in the supplied database\n\n\nmongo-oplog-collection-name\n\n\nstring (default $oplog.main)\n\n\nWhen mongo-oplog-collection-name is given monstache will look for the mongodb oplog in the supplied collection\n\n\nmongo-cursor-timeout\n\n\nstring (default 100s)\n\n\nWhen mongo-cursor-timeout is given monstache will time out and re-query the oplog after the supplied duration.\nDuration values are expected in the form 50s.\n\n\nmongo-dial-settings\n\n\nTOML table (default nil)\n\n\nThe following mongodb dial properties are available\n\n\n\n\nssl\n\n\nbool (default false)\n\n\nSet to true to establish a connection using TLS.\n\n\ntimeout\n\n\nint (default 10)\n\n\nSeconds to wait when establishing a connection to mongodb before giving up\n\n\n\n\nmongo-session-settings\n\n\nTOML table (default nil)\n\n\nThe following mongodb session properties are available\n\n\n\n\nsocket-timeout\n\n\nint (default 60)\n\n\nSeconds to wait for a non-responding socket before it is forcefully closed\n\n\nsync-timeout\n\n\nint (default 60)\n\n\nAmount of time in seconds an operation will wait before returning an error in case a connection to a usable server can't be established.\nSet it to zero to wait forever.\n\n\n\n\ngtm-settings\n\n\nTOML table (default nil)\n\n\nThe following gtm configuration properties are available.  See \ngtm\n for details\n\n\n\n\nchannel-size\n\n\nint (default 512)\n\n\nControls the size of the go channels created for processing events.  When many events\nare processed at once a larger channel size may prevent blocking in gtm.\n\n\nbuffer-size\n\n\nint (default 32)\n\n\nDetermines how many documents are buffered by a gtm worker go routine before they are batch fetched from\nmongodb.  When many documents are inserted or updated at once it is better to fetch them together.\n\n\nbuffer-duration\n\n\nstring (default 750ms)\n\n\nA string representation of a golang duration.  Determines the maximum time a buffer is held before it is \nfetched in batch from mongodb and flushed for indexing.\n\n\n\n\nindex-files\n\n\nboolean (default false)\n\n\nWhen index-files is true monstache will index the raw content of files stored in GridFS into elasticsearch as an attachment type.\nBy default index-files is false meaning that monstache will only index metadata associated with files stored in GridFS.\nIn order for index-files to index the raw content of files stored in GridFS you must install a plugin for elasticsearch.\nFor versions of elasticsearch prior to version 5, you should install the \nmapper-attachments\n plugin.  In version 5 or greater\nof elasticsearch the mapper-attachment plugin is deprecated and you should install the \ningest-attachment\n plugin instead.\nFor further information on how to configure monstache to index content from GridFS, see the section \nGridFS support\n.\n\n\nmax-file-size\n\n\nint (default 0)\n\n\nWhen max-file-size is greater than 0 monstache will not index the content of GridFS files that exceed this limit in bytes.\n\n\nfile-namespaces\n\n\n[]string (default nil)\n\n\nThe file-namespaces config must be set when index-files is enabled.  file-namespaces must be set to an array of mongodb\nnamespace strings.  Files uploaded through gridfs to any of the namespaces in file-namespaces will be retrieved and their\nraw content indexed into elasticsearch via either the mapper-attachments or ingest-attachment plugin. \n\n\nThis option may be passed on the command line as ./monstache --file-namespace test.foo --file-namespace test.bar\n\n\nfile-highlighting\n\n\nboolean (default false)\n\n\nWhen file-highlighting is true monstache will enable the ability to return highlighted keywords in the extracted text of files\nfor queries on files which were indexed in elasticsearch from gridfs.\n\n\nverbose\n\n\nboolean (default false)\n\n\nWhen verbose is true monstache with enable debug logging including a trace of requests to elasticsearch\n\n\nelasticsearch-user\n\n\nstring (default \"\")\n\n\nOptional Elasticsearch username for basic auth\n\n\nelasticsearch-password\n\n\nstring (default \"\")\n\n\nOptional Elasticsearch password for basic auth\n\n\nelasticsearch-urls\n\n\n[]string (default \n[ \"http://localhost:9200\" ]\n)\n\n\nAn array of URLs to connect to the Elasticsearch REST Interface\n\n\nThis option may be passed on the command line as ./monstache --elasticsearch-url URL1 --elasticsearch-url URL2 \n\n\nelasticsearch-version\n\n\nstring (by default determined by connecting to the server)\n\n\nWhen elasticsearch-version is provided monstache will parse the given server version to determine how to interact with\nthe elasticsearch API.  This is normally not recommended because monstache will connect to elasticsearch to find out\nwhich version is being used.  This option is provided for cases where connecting to the base URL of the elasticsearch REST\nAPI to get the version is not possible or desired.\n\n\nelasticsearch-max-conns\n\n\nint (default 4)\n\n\nThe size of the Elasticsearch HTTP connection pool. This determines the concurrency of bulk indexing requests to Elasticsearch.\nIf you increase this value too high you may begin to see bulk indexing failures if the bulk index queue gets overloaded.\nTo increase the size of the bulk indexing queue you can update the Elasticsearch config file:\n\n\nthread_pool:\n    bulk:\n    queue_size: 200\n\n\n\nFor more information see \nThread Pool\n.\n\n\nYou will want to tune this variable in sync with the \nelasticsearch-max-bytes\n option.\n\n\nelasticsearch-retry\n\n\nboolean (default false)\n\n\nWhen elasticseach-retry is true a failed request to elasticsearch will be retried with an exponential backoff policy. The policy\nis set with an initial timeout of 50 ms, an exponential factor of 2, and a max wait of 20 seconds. For more information on how \nthis works see \nBack Off Strategy\n\n\nelasticsearch-client-timeout\n\n\nint (default 60)\n\n\nThe number of seconds before a request to elasticsearch times out\n\n\nelasticsearch-max-docs\n\n\nint (default -1)\n\n\nWhen elasticsearch-max-docs is given a bulk index request to elasticsearch will be forced when the buffer reaches the given number of documents.\n\n\n\n\nWarning\n\n\nIt is not recommended to change this option but rather use \nelasticsearch-max-bytes\n instead since the document count is not a good gauge of when\nto flush.  The default value of -1 means to not use the number of docs as a flush indicator. \n\n\n\n\nelasticsearch-max-bytes\n\n\nint (default 8MB as bytes)\n\n\nWhen elasticsearch-max-bytes is given a bulk index request to elasticsearch will be forced when a connection buffer reaches the given number of bytes. This\nsetting greatly impacts performance. A high value for this setting will cause high memory monstache memory usage as the documents are buffered in memory.\n\n\nEach connection in \nelasticsearch-max-conns\n will flush when its queue gets filled to this size. \n\n\nelasticsearch-max-seconds\n\n\nint (default 1)\n\n\nWhen elasticsearch-max-seconds is given a bulk index request to elasticsearch will be forced when a request has not been made in the given number of seconds.\nThe default value is automatically increased to \n5\n when direct read namespaces are detected.  This is to ensure that flushes do not happen too often in this\ncase which would cut performance.\n\n\nelasticsearch-pem-file\n\n\nstring (default \"\")\n\n\nWhen elasticsearch-pem-file is given monstache will use the given file path to add a local certificate to x509 cert\npool when connecting to elasticsearch. This should only be used when elasticsearch is configured with SSL enabled.\n\n\nelasticsearch-validate-pem\n\n\nboolean (default true)\n\n\nWhen elasticsearch-validate-pem-file is false TLS will be configured to skip verification\n\n\ndropped-databases\n\n\nboolean (default true)\n\n\nWhen dropped-databases is false monstache will not delete the mapped indexes in elasticsearch if a mongodb database is dropped\n\n\ndropped-collections\n\n\nboolean (default true)\n\n\nWhen dropped-collections is false monstache will not delete the mapped index in elasticsearch if a mongodb collection is dropped\n\n\nworker\n\n\nstring (default \"\")\n\n\nWhen worker is given monstache will enter multi-worker mode and will require you to also provide the config option workers.  Use this mode to run\nmultiple monstache processes and distribute the work between them.  In this mode monstache will ensure that each mongo document id always goes to the\nsame worker and none of the other workers. See the section \nworkers\n for more information.\n\n\nworkers\n\n\n[]string (default nil)\n\n\nAn array of worker names to be used in conjunction with the worker option. \n\n\nThis option may be passed on the command line as ./monstache --workers w1 --workers w2 \n\n\nenable-patches\n\n\nboolean (default false) \n\n\nSet to true to enable storing \nrfc7396\n patches in your elasticsearch documents\n\n\npatch-namespaces\n\n\n[]string (default nil)\n\n\nAn array of mongodb namespaces that you would like to enable rfc7396 patches on\n\n\nThis option may be passed on the command line as ./monstache --patch-namespace test.foo --patch-namespace test.bar \n\n\nmerge-patch-attribute\n\n\nstring (default \"json-merge-patches\") \n\n\nCustomize the name of the property under which merge patches are stored\n\n\ncluster-name\n\n\nstring (default \"\")\n\n\nWhen cluster-name is given monstache will enter a high availablity mode. Processes with cluster name set to the same value will coordinate.  Only one of the\nprocesses in a cluster will sync changes.  The other processes will be in a paused state.  If the process which is syncing changes goes down for some reason\none of the processes in paused state will take control and start syncing.  See the section \nhigh availability\n for more information.\n\n\nfilter\n\n\n[] array of TOML table (default nil)\n\n\nWhen filter is given monstache will pass the mongodb document from an insert or update operation into the filter function immediately after it is read from the oplog.  Return true from the function to continue processing the document or false to completely ignore the document. See the section \nMiddleware\n for more information.\n\n\n\n\nnamespace\n\n\nstring (default \"\")\n\n\nThe MongoDB namespace, db.collection, to apply the script to\n\n\nscript\n\n\nstring (default \"\")\n\n\nAn inline script.  You can use TOML multiline syntax here\n\n\npath\n\n\nstring (default \"\")\n\n\nThe file path to load a script from.  Use this or an inline script but not both. Can be a\npath relative to the directory monstache is executed from or an absolute path.\n\n\n\n\nscript\n\n\n[] array of TOML table (default nil)\n\n\nWhen script is given monstache will pass the mongodb document into the script before indexing into elasticsearch.  See the section \nMiddleware\n\nfor more information.\n\n\n\n\nnamespace\n\n\nstring (default \"\")\n\n\nThe MongoDB namespace, db.collection, to apply the script to\n\n\nrouting\n\n\nboolean (default false)\n\n\nSet routing to true if you override the index, routing or parent metadata via _meta_monstache\n\n\nscript\n\n\nstring (default \"\")\n\n\nAn inline script.  You can use TOML multiline syntax here\n\n\npath\n\n\nstring (default \"\")\n\n\nThe file path to load a script from.  Use this or an inline script but not both. Can be a\npath relative to the directory monstache is executed from or an absolute path.\n\n\n\n\ngraylog-addr\n\n\nstring (default \"\")\n\n\nThe address of a graylog server to redirect logs to in GELF \n\n\nlogs\n\n\nTOML table (default nil)\n\n\nAllows writing logs to a file using a rolling appender instead of stdout.  Supply a file path for each type of log you would like to send to a file.\n\n\n\n\ninfo\n\n\nstring (default \"\")\n\n\nThe file path to write info level logs to\n\n\nerror\n\n\nstring (default \"\")\n\n\nThe file path to write error level logs to\n\n\ntrace\n\n\nstring (default \"\")\n\n\nThe file path to write trace level logs to. Trace logs are enabled via the verbose option.\n\n\nstats\n\n\nstring (default \"\")\n\n\nThe file path to write indexing statistics to. Stats logs are enabled via the stats option.\n\n\n\n\nenable-http-server\n\n\nboolean (default false)\n\n\nAdd this flag to enable an embedded HTTP server at localhost:8080\n\n\nhttp-server-addr\n\n\nstring (default \":8080\")\n\n\nThe address to bind the embedded HTTP server on if enabled", 
            "title": "Configuration"
        }, 
        {
            "location": "/config/#configuration", 
            "text": "Configuration can be specified in your TOML config file or be passed into monstache as Go program arguments on the command line.\nProgram arguments take precedance over configuration specified in the TOML config file.   Warning  Please keep any simple -one line config- above any  [[script]]  or toml table configs, as  there is a bug  in the toml parser where if you have definitions below a TOML table, e.g. a  [[script]]  then the parser thinks that lines below that belong to the table instead of at the global level", 
            "title": "Configuration"
        }, 
        {
            "location": "/config/#print-config", 
            "text": "boolean (default false)  When print-config is true monstache will print its configuration and then exit", 
            "title": "print-config"
        }, 
        {
            "location": "/config/#stats", 
            "text": "boolean (default false)  When stats is true monstache will periodically print statistics accumulated by the indexer", 
            "title": "stats"
        }, 
        {
            "location": "/config/#stats-duration", 
            "text": "string (default 30s)  Sets the duration after which statistics are printed if stats is enabled", 
            "title": "stats-duration"
        }, 
        {
            "location": "/config/#index-stats", 
            "text": "boolean (default false)  When index-stats is true monstache will write statistics about its indexing progress in\nElasticsearch.  The indexes used to store the statistics are time stamped by day and \nprefixed  monstache.stats. . E.g. monstache.stats.2017-07-01 and so on.   As these indexes will accrue over time your can use a tool like  curator \nto prune them with a Delete Indices action and an age filter.", 
            "title": "index-stats"
        }, 
        {
            "location": "/config/#stats-index-format", 
            "text": "string (default monstache.stats.2006-01-02)  The  time.Time  supported index name format for stats indices.  By default, stats indexes \nare partitioned by day.  To use less indices for stats you can shorten this format string \n(e.g monstache.stats.2006-01) or remove the time component completely to use a single index.", 
            "title": "stats-index-format"
        }, 
        {
            "location": "/config/#gzip", 
            "text": "boolean (default false)  When gzip is true, monstache will compress requests to elasticsearch to increase performance. \nIf you enable gzip in monstache and are using elasticsearch prior to version 5 you will also \nneed to update the elasticsearch config file to set http.compression: true. In elasticsearch \nversion 5 and above http.compression is enabled by default. Enabling gzip is recommended \nespecially if you enable the index-files setting.", 
            "title": "gzip"
        }, 
        {
            "location": "/config/#fail-fast", 
            "text": "boolean (default false)  When fail-fast is true, if monstache receives a failed bulk indexing response from Elasticsearch, monstache\nwill log the request that produced the response as an ERROR and then exit immediately with an error status. \nNormally, monstache just logs the error and continues processing events.  If monstache has been configured with  elasticsearch-retry  true, a failed request will be\nretried before being considered a failure.", 
            "title": "fail-fast"
        }, 
        {
            "location": "/config/#prune-invalid-json", 
            "text": "boolean (default false)  If you MongoDB data contains values like +Infinity, -Infinity, NaN, or invalid dates you will want to set this option to true.  The\nGolang json serializer is not able to handle these values and the indexer will get stuck in an infinite loop. When this\nis set to true Monstache will drop those fields so that indexing errors do not occur.", 
            "title": "prune-invalid-json"
        }, 
        {
            "location": "/config/#index-oplog-time", 
            "text": "boolean (default false)  If this option is set to true monstache will include 2 automatic fields in the source document indexed into\nElasticsearch.  The first is _oplog_ts which is the timestamp for the event copied directly from the MongoDB\noplog. The second is _oplog_date which is an Elasticsearch date field corresponding to the time of the same\nevent.  This information is generally useful in Elasticsearch giving the notion of last updated.  However, it's also\nvaluable information to have for failed indexing requests since it gives one the information to replay from \na failure point.  See the option  resume-from-timestamp  for information on how to replay oplog events since \na given event occurred.   For data read via the direct read feature the oplog time will only be available if the id of the MongoDB\ndocument is an ObjectID.  If the id of the MongoDB document is not an ObjectID and the document source is\na direct read query then the oplog time with not be available.", 
            "title": "index-oplog-time"
        }, 
        {
            "location": "/config/#resume", 
            "text": "boolean (default false)  When resume is true, monstache writes the timestamp of mongodb operations it has successfully synced to elasticsearch\nto the collection monstache.monstache.  It also reads the timestamp from that collection when it starts in order to replay\nevents which it might have missed because monstache was stopped. If monstache is started with the  cluster-name  option\nset then resume is automatically turned on.", 
            "title": "resume"
        }, 
        {
            "location": "/config/#resume-name", 
            "text": "string (default \"default\")  monstache uses the value of resume-name as an id when storing and retrieving timestamps\nto and from the mongodb collection monstache.monstache. The default value for this option is  default .\nHowever, there are some exceptions.  If monstache is started with the  cluster-name  option set then the\nname of the cluster becomes the resume-name.  This is to ensure that any process in the cluster is able to resume\nfrom the last timestamp successfully processed.  The other exception occurs when resume-name is not given but worker-name  is.  In that cause the worker name becomes the resume-name.", 
            "title": "resume-name"
        }, 
        {
            "location": "/config/#resume-from-timestamp", 
            "text": "int64 (default 0)  When resume-from-timestamp (a 64 bit timestamp where the high 32 bytes represent the number of seconds since epoch and the low 32 bits\nrepresent an offset within a second) is given, monstache will sync events starting immediately after the timestamp.  This is useful if you have \na specific timestamp from the oplog and would like to start syncing from after this event.", 
            "title": "resume-from-timestamp"
        }, 
        {
            "location": "/config/#replay", 
            "text": "boolean (default false)  When replay is true, monstache replays all events from the beginning of the mongodb oplog and syncs them to elasticsearch.  When  resume  and replay are both true, monstache replays all events from the beginning of the mongodb oplog, syncs them\nto elasticsearch and also writes the timestamp of processed events to monstache.monstache.   When neither resume nor replay are true, monstache reads the last timestamp in the oplog and starts listening for events\noccurring after this timestamp.  Timestamps are not written to monstache.monstache.  This is the default behavior.", 
            "title": "replay"
        }, 
        {
            "location": "/config/#resume-write-unsafe", 
            "text": "boolean (default false)  When resume-write-unsafe is true monstache sets the safety mode of the mongodb session such that writes are fire and forget.\nThis speeds up writing of timestamps used to resume synching in a subsequent run of monstache.  This speed up comes at the cost\nof no error checking on the write of the timestamp.  Since errors writing the last synched timestamp are only logged by monstache\nand do not stop execution it's not unreasonable to set this to true to get a speedup.", 
            "title": "resume-write-unsafe"
        }, 
        {
            "location": "/config/#time-machine-namespaces", 
            "text": "[]string (default nil)  Monstache is good at keeping your MongoDB collections and Elasticsearch indexes in sync.  When a document is updated in MongoDB the corresponding document in Elasticsearch is updated too.  Same goes for deleting documents in MongoDB.  But what if you also wanted to keep a log of all the changes to a MongoDB document over its lifespan.  That's what time-machine-namespaces are for.  When you configure a list of namespaces in MongoDB to add to the time machine, in addition to keeping documents in sync, Monstache will index of copy of your MongoDB document at the time it changes in a separate timestamped index.  Say for example, you insert a document into the  test.test  collection in MongoDB.  Monstache will index by default into the  test.test  index in Elasticsearch, but with time machines it will also index it into  log.test.test.2018-02-19 .  When it indexes it into the time machine index it does so without the id from MongoDB and lets Elasticsearch generate a unique id.  But, it stores the id from MongoDB in the source field  _source_id .  Also, it adds _oplog_ts and _oplog_date fields on the source document.  These correspond to the timestamp from the oplog when the data changed in MongoDB. Finally, it routes the document by the MongoDB id so that you can speed up queries later to find changes to a doc.  This lets you do some cool things but mostly you'll want to sort by  _oplog_date  and filter by  _source_id  to see how documents have changed over time.   Because the indexes are timestamped you can drop then after a period of time so they don't take up space.  If you just want the last couple of days of changes, delete the indexes with the old timestamps.  Elastic  curator  is your friend here.  This option may be passed on the command line as ./monstache --time-machine-namespace test.foo --time-machine-namespace test.bar", 
            "title": "time-machine-namespaces"
        }, 
        {
            "location": "/config/#time-machine-index-prefix", 
            "text": "string (default \"log\")  If you have enabled time machine namespaces and want to change the prefix assigned to the index names use this setting.", 
            "title": "time-machine-index-prefix"
        }, 
        {
            "location": "/config/#time-machine-index-suffix", 
            "text": "string (default \"2006-01-02\")  If you have enabled time machine namespaces and want to suffix the index names using a different date format use this setting.  Consult the golang docs for how date formats work.  By default this suffixes the index name with the year, month, and day.", 
            "title": "time-machine-index-suffix"
        }, 
        {
            "location": "/config/#time-machine-direct-reads", 
            "text": "boolean (default false)  This setting controls whether or not direct reads are added to the time machine log index. This is false by default so only changes read from the oplog are added.", 
            "title": "time-machine-direct-reads"
        }, 
        {
            "location": "/config/#routing-namespaces", 
            "text": "[]string (default nil)  You only need to set this configuration option if you use golang and javascript plugins are do custom routing: override parent or routing attributes. This array should be set to a list of all the namespaces that custom routing is done on. This ensures that deletes in MongoDB are routed correctly to \nElasticsearch.", 
            "title": "routing-namespaces"
        }, 
        {
            "location": "/config/#delete-strategy", 
            "text": "int (default 0)  The strategy to use for handling document deletes when custom indexing is done in scripts.   Warning  Breaking change in versions  v4.4.0     v3.11.0 . Monstache was saving routing foreach document in mongodb in a db called  monstache  collection  meta  using the same mongodb URL and credentials provided in config by default.\nMonstache was only saving this information if the document metadata was being altered via  _monstache_meta  in a script or via the API in a golang plugin.  Monstache needed to save this information in order to locate and perform deletes in Elasticsearch if the corresponding document was deleted in MongoDB.\nIf you want to maintain this stategy use value 1.  Otherwise, you can drop the  monstache.meta  collection as this is no longer used by default.   But now this has changed to be stateless, you can read more:  discussion     commit  Strategy 0  -default- will do a term query by document id across all Elasticsearch indexes. Will only perform the delete if one single document is returned by the query.  Stategy 1  will store indexing metadata in MongoDB in the  monstache.meta  collection and use this metadata to locate and delete the document.  Stategy 2  will completely ignore document deletes in MongoDB.", 
            "title": "delete-strategy"
        }, 
        {
            "location": "/config/#delete-index-pattern", 
            "text": "string (default *)  When using a stateless delete strategy, set this to a valid Elasticsearch index pattern to restrict the scope of possible indexes that a stateless delete\nwill consider.  If monstache only indexes to index a, b, and c then you can set this to  a,b,c .  If monstache only indexes to indexes starting with \nmydb then you can set this to  mydb* .", 
            "title": "delete-index-pattern"
        }, 
        {
            "location": "/config/#direct-read-namespaces", 
            "text": "[]string (default nil)  At times even being able to replay from the beginning of the oplog is not enough to sync all of your mongodb data.\nThe oplog is a capped collection and may only contain a subset of the data.  In this case you can perform a direct\nsync of mongodb to elasticsearch.  To do this, set direct-read-namespaces to an array of namespaces that you would \nlike to copy.  Monstache will perform reads directly from the given set of db.collection and sync them to elasticsearch.  This option may be passed on the command line as ./monstache --direct-read-namespace test.foo --direct-read-namespace test.bar  By default, Monstache maps a MongoDB collection named  foo  in a database named  test  to the  test.foo  index in Elasticsearch.  For maximum indexing performance when doing alot of a direct reads you will want to adjust the refresh interval during indexing on the\ndestination Elasticsearch indices.  The refresh interval can be set at a global level in elasticsearch.yml or on a per\nindex basis by using the Index Settings or Index Template APIs.  For more information see  Update Indices Settings .  By default, Elasticsearch refreshes every second.  You will want to increase this value or turn off refresh completely during the indexing\nphase by setting the refresh_interval to -1.  Remember to reset the refresh_interval to a positive value and do a force merge after the indexing \nphase has completed if you decide to temporarily turn off refresh, otherwise you will not be able to see the new documents in queries.", 
            "title": "direct-read-namespaces"
        }, 
        {
            "location": "/config/#direct-read-split-max", 
            "text": "int (default 9)  The maximum number of times to split a collection for direct reads.  This setting greatly impacts the memory consumption\nof Monstache.  When direct reads are performed, the collection is first broken up into ranges which are then read \nconcurrently is separate go routines.  If you increase this value you will notice the connection count increase in mongostat\nwhen direct reads are performed.  You will also notice the memory consumption of Monstache grow.  Increasing this value can\nincrease the throughput for reading large collections, but you need to have enough memory available to Monstache to do so.\nYou can decrease this value for a memory constrained Monstache process.", 
            "title": "direct-read-split-max"
        }, 
        {
            "location": "/config/#exit-after-direct-reads", 
            "text": "boolean (default false)  The  direct-read-namespaces  option gives you a way to do a full sync on multiple collections.  At times you may want\nto perform a full sync via the direct-read-namespaces option and then quit monstache.  Set this option to true and\nmonstache will exit after syncing the direct read collections instead of continuing to tail the oplog. This is useful\nif you would like to run monstache to run a full sync on a set of collections via a cron job.", 
            "title": "exit-after-direct-reads"
        }, 
        {
            "location": "/config/#namespace-regex", 
            "text": "regexp (default \"\")  When namespace-regex is given this regex is tested against the namespace, database.collection, of the event. If\nthe regex matches monstache continues processing event filters, otherwise it drops the event. By default monstache\nprocesses events in all databases and all collections with the exception of the reserved database monstache, any\ncollections suffixed with .chunks, and the system collections. For more information see the section  Namespaces .", 
            "title": "namespace-regex"
        }, 
        {
            "location": "/config/#namespace-exclude-regex", 
            "text": "regex (default \"\")  When namespace-exclude-regex is given this regex is tested against the namespace, database.collection, of the event. If\nthe regex matches monstache ignores the event, otherwise it continues processing event filters. By default monstache\nprocesses events in all databases and all collections with the exception of the reserved database monstache, any\ncollections suffixed with .chunks, and the system collections. For more information see the section  Namespaces .", 
            "title": "namespace-exclude-regex"
        }, 
        {
            "location": "/config/#mongo-url", 
            "text": "string (default localhost)  The URL to connect to MongoDB which must follow the  Standard Connection String Format  For sharded clusters this URL should point to the  mongos  router server and the  mongo-config-url \noption should be set to point to the config server.", 
            "title": "mongo-url"
        }, 
        {
            "location": "/config/#mongo-config-url", 
            "text": "string (default \"\")  This config should only be set for sharded MongoDB clusters. Has the same syntax as mongo-url.\nThis URL should point to the MongoDB  config  server.  Monstache will read the list of shards using this connection and then setup a listener to react\nto new shards being added to the cluster at a later time.", 
            "title": "mongo-config-url"
        }, 
        {
            "location": "/config/#mongo-pem-file", 
            "text": "string (default \"\")  When mongo-pem-file is given monstache will use the given file path to add a local certificate to x509 cert\npool when connecting to mongodb. This should only be used when mongodb is configured with SSL enabled.", 
            "title": "mongo-pem-file"
        }, 
        {
            "location": "/config/#mongo-validate-pem", 
            "text": "boolean (default true)  When mongo-validate-pem-file is false TLS will be configured to skip verification", 
            "title": "mongo-validate-pem"
        }, 
        {
            "location": "/config/#mongo-oplog-database-name", 
            "text": "string (default local)  When mongo-oplog-database-name is given monstache will look for the mongodb oplog in the supplied database", 
            "title": "mongo-oplog-database-name"
        }, 
        {
            "location": "/config/#mongo-oplog-collection-name", 
            "text": "string (default $oplog.main)  When mongo-oplog-collection-name is given monstache will look for the mongodb oplog in the supplied collection", 
            "title": "mongo-oplog-collection-name"
        }, 
        {
            "location": "/config/#mongo-cursor-timeout", 
            "text": "string (default 100s)  When mongo-cursor-timeout is given monstache will time out and re-query the oplog after the supplied duration.\nDuration values are expected in the form 50s.", 
            "title": "mongo-cursor-timeout"
        }, 
        {
            "location": "/config/#mongo-dial-settings", 
            "text": "TOML table (default nil)  The following mongodb dial properties are available", 
            "title": "mongo-dial-settings"
        }, 
        {
            "location": "/config/#ssl", 
            "text": "", 
            "title": "ssl"
        }, 
        {
            "location": "/config/#bool-default-false", 
            "text": "Set to true to establish a connection using TLS.", 
            "title": "bool (default false)"
        }, 
        {
            "location": "/config/#timeout", 
            "text": "", 
            "title": "timeout"
        }, 
        {
            "location": "/config/#int-default-10", 
            "text": "Seconds to wait when establishing a connection to mongodb before giving up", 
            "title": "int (default 10)"
        }, 
        {
            "location": "/config/#mongo-session-settings", 
            "text": "TOML table (default nil)  The following mongodb session properties are available", 
            "title": "mongo-session-settings"
        }, 
        {
            "location": "/config/#socket-timeout", 
            "text": "int (default 60)  Seconds to wait for a non-responding socket before it is forcefully closed", 
            "title": "socket-timeout"
        }, 
        {
            "location": "/config/#sync-timeout", 
            "text": "int (default 60)  Amount of time in seconds an operation will wait before returning an error in case a connection to a usable server can't be established.\nSet it to zero to wait forever.", 
            "title": "sync-timeout"
        }, 
        {
            "location": "/config/#gtm-settings", 
            "text": "TOML table (default nil)  The following gtm configuration properties are available.  See  gtm  for details", 
            "title": "gtm-settings"
        }, 
        {
            "location": "/config/#channel-size", 
            "text": "int (default 512)  Controls the size of the go channels created for processing events.  When many events\nare processed at once a larger channel size may prevent blocking in gtm.", 
            "title": "channel-size"
        }, 
        {
            "location": "/config/#buffer-size", 
            "text": "int (default 32)  Determines how many documents are buffered by a gtm worker go routine before they are batch fetched from\nmongodb.  When many documents are inserted or updated at once it is better to fetch them together.", 
            "title": "buffer-size"
        }, 
        {
            "location": "/config/#buffer-duration", 
            "text": "string (default 750ms)  A string representation of a golang duration.  Determines the maximum time a buffer is held before it is \nfetched in batch from mongodb and flushed for indexing.", 
            "title": "buffer-duration"
        }, 
        {
            "location": "/config/#index-files", 
            "text": "boolean (default false)  When index-files is true monstache will index the raw content of files stored in GridFS into elasticsearch as an attachment type.\nBy default index-files is false meaning that monstache will only index metadata associated with files stored in GridFS.\nIn order for index-files to index the raw content of files stored in GridFS you must install a plugin for elasticsearch.\nFor versions of elasticsearch prior to version 5, you should install the  mapper-attachments  plugin.  In version 5 or greater\nof elasticsearch the mapper-attachment plugin is deprecated and you should install the  ingest-attachment  plugin instead.\nFor further information on how to configure monstache to index content from GridFS, see the section  GridFS support .", 
            "title": "index-files"
        }, 
        {
            "location": "/config/#max-file-size", 
            "text": "int (default 0)  When max-file-size is greater than 0 monstache will not index the content of GridFS files that exceed this limit in bytes.", 
            "title": "max-file-size"
        }, 
        {
            "location": "/config/#file-namespaces", 
            "text": "[]string (default nil)  The file-namespaces config must be set when index-files is enabled.  file-namespaces must be set to an array of mongodb\nnamespace strings.  Files uploaded through gridfs to any of the namespaces in file-namespaces will be retrieved and their\nraw content indexed into elasticsearch via either the mapper-attachments or ingest-attachment plugin.   This option may be passed on the command line as ./monstache --file-namespace test.foo --file-namespace test.bar", 
            "title": "file-namespaces"
        }, 
        {
            "location": "/config/#file-highlighting", 
            "text": "boolean (default false)  When file-highlighting is true monstache will enable the ability to return highlighted keywords in the extracted text of files\nfor queries on files which were indexed in elasticsearch from gridfs.", 
            "title": "file-highlighting"
        }, 
        {
            "location": "/config/#verbose", 
            "text": "boolean (default false)  When verbose is true monstache with enable debug logging including a trace of requests to elasticsearch", 
            "title": "verbose"
        }, 
        {
            "location": "/config/#elasticsearch-user", 
            "text": "string (default \"\")  Optional Elasticsearch username for basic auth", 
            "title": "elasticsearch-user"
        }, 
        {
            "location": "/config/#elasticsearch-password", 
            "text": "string (default \"\")  Optional Elasticsearch password for basic auth", 
            "title": "elasticsearch-password"
        }, 
        {
            "location": "/config/#elasticsearch-urls", 
            "text": "[]string (default  [ \"http://localhost:9200\" ] )  An array of URLs to connect to the Elasticsearch REST Interface  This option may be passed on the command line as ./monstache --elasticsearch-url URL1 --elasticsearch-url URL2", 
            "title": "elasticsearch-urls"
        }, 
        {
            "location": "/config/#elasticsearch-version", 
            "text": "string (by default determined by connecting to the server)  When elasticsearch-version is provided monstache will parse the given server version to determine how to interact with\nthe elasticsearch API.  This is normally not recommended because monstache will connect to elasticsearch to find out\nwhich version is being used.  This option is provided for cases where connecting to the base URL of the elasticsearch REST\nAPI to get the version is not possible or desired.", 
            "title": "elasticsearch-version"
        }, 
        {
            "location": "/config/#elasticsearch-max-conns", 
            "text": "int (default 4)  The size of the Elasticsearch HTTP connection pool. This determines the concurrency of bulk indexing requests to Elasticsearch.\nIf you increase this value too high you may begin to see bulk indexing failures if the bulk index queue gets overloaded.\nTo increase the size of the bulk indexing queue you can update the Elasticsearch config file:  thread_pool:\n    bulk:\n    queue_size: 200  For more information see  Thread Pool .  You will want to tune this variable in sync with the  elasticsearch-max-bytes  option.", 
            "title": "elasticsearch-max-conns"
        }, 
        {
            "location": "/config/#elasticsearch-retry", 
            "text": "boolean (default false)  When elasticseach-retry is true a failed request to elasticsearch will be retried with an exponential backoff policy. The policy\nis set with an initial timeout of 50 ms, an exponential factor of 2, and a max wait of 20 seconds. For more information on how \nthis works see  Back Off Strategy", 
            "title": "elasticsearch-retry"
        }, 
        {
            "location": "/config/#elasticsearch-client-timeout", 
            "text": "int (default 60)  The number of seconds before a request to elasticsearch times out", 
            "title": "elasticsearch-client-timeout"
        }, 
        {
            "location": "/config/#elasticsearch-max-docs", 
            "text": "int (default -1)  When elasticsearch-max-docs is given a bulk index request to elasticsearch will be forced when the buffer reaches the given number of documents.   Warning  It is not recommended to change this option but rather use  elasticsearch-max-bytes  instead since the document count is not a good gauge of when\nto flush.  The default value of -1 means to not use the number of docs as a flush indicator.", 
            "title": "elasticsearch-max-docs"
        }, 
        {
            "location": "/config/#elasticsearch-max-bytes", 
            "text": "int (default 8MB as bytes)  When elasticsearch-max-bytes is given a bulk index request to elasticsearch will be forced when a connection buffer reaches the given number of bytes. This\nsetting greatly impacts performance. A high value for this setting will cause high memory monstache memory usage as the documents are buffered in memory.  Each connection in  elasticsearch-max-conns  will flush when its queue gets filled to this size.", 
            "title": "elasticsearch-max-bytes"
        }, 
        {
            "location": "/config/#elasticsearch-max-seconds", 
            "text": "int (default 1)  When elasticsearch-max-seconds is given a bulk index request to elasticsearch will be forced when a request has not been made in the given number of seconds.\nThe default value is automatically increased to  5  when direct read namespaces are detected.  This is to ensure that flushes do not happen too often in this\ncase which would cut performance.", 
            "title": "elasticsearch-max-seconds"
        }, 
        {
            "location": "/config/#elasticsearch-pem-file", 
            "text": "string (default \"\")  When elasticsearch-pem-file is given monstache will use the given file path to add a local certificate to x509 cert\npool when connecting to elasticsearch. This should only be used when elasticsearch is configured with SSL enabled.", 
            "title": "elasticsearch-pem-file"
        }, 
        {
            "location": "/config/#elasticsearch-validate-pem", 
            "text": "boolean (default true)  When elasticsearch-validate-pem-file is false TLS will be configured to skip verification", 
            "title": "elasticsearch-validate-pem"
        }, 
        {
            "location": "/config/#dropped-databases", 
            "text": "boolean (default true)  When dropped-databases is false monstache will not delete the mapped indexes in elasticsearch if a mongodb database is dropped", 
            "title": "dropped-databases"
        }, 
        {
            "location": "/config/#dropped-collections", 
            "text": "boolean (default true)  When dropped-collections is false monstache will not delete the mapped index in elasticsearch if a mongodb collection is dropped", 
            "title": "dropped-collections"
        }, 
        {
            "location": "/config/#worker", 
            "text": "string (default \"\")  When worker is given monstache will enter multi-worker mode and will require you to also provide the config option workers.  Use this mode to run\nmultiple monstache processes and distribute the work between them.  In this mode monstache will ensure that each mongo document id always goes to the\nsame worker and none of the other workers. See the section  workers  for more information.", 
            "title": "worker"
        }, 
        {
            "location": "/config/#workers", 
            "text": "[]string (default nil)  An array of worker names to be used in conjunction with the worker option.   This option may be passed on the command line as ./monstache --workers w1 --workers w2", 
            "title": "workers"
        }, 
        {
            "location": "/config/#enable-patches", 
            "text": "boolean (default false)   Set to true to enable storing  rfc7396  patches in your elasticsearch documents", 
            "title": "enable-patches"
        }, 
        {
            "location": "/config/#patch-namespaces", 
            "text": "[]string (default nil)  An array of mongodb namespaces that you would like to enable rfc7396 patches on  This option may be passed on the command line as ./monstache --patch-namespace test.foo --patch-namespace test.bar", 
            "title": "patch-namespaces"
        }, 
        {
            "location": "/config/#merge-patch-attribute", 
            "text": "string (default \"json-merge-patches\")   Customize the name of the property under which merge patches are stored", 
            "title": "merge-patch-attribute"
        }, 
        {
            "location": "/config/#cluster-name", 
            "text": "string (default \"\")  When cluster-name is given monstache will enter a high availablity mode. Processes with cluster name set to the same value will coordinate.  Only one of the\nprocesses in a cluster will sync changes.  The other processes will be in a paused state.  If the process which is syncing changes goes down for some reason\none of the processes in paused state will take control and start syncing.  See the section  high availability  for more information.", 
            "title": "cluster-name"
        }, 
        {
            "location": "/config/#filter", 
            "text": "[] array of TOML table (default nil)  When filter is given monstache will pass the mongodb document from an insert or update operation into the filter function immediately after it is read from the oplog.  Return true from the function to continue processing the document or false to completely ignore the document. See the section  Middleware  for more information.", 
            "title": "filter"
        }, 
        {
            "location": "/config/#namespace", 
            "text": "string (default \"\")  The MongoDB namespace, db.collection, to apply the script to", 
            "title": "namespace"
        }, 
        {
            "location": "/config/#script", 
            "text": "string (default \"\")  An inline script.  You can use TOML multiline syntax here", 
            "title": "script"
        }, 
        {
            "location": "/config/#path", 
            "text": "string (default \"\")  The file path to load a script from.  Use this or an inline script but not both. Can be a\npath relative to the directory monstache is executed from or an absolute path.", 
            "title": "path"
        }, 
        {
            "location": "/config/#script_1", 
            "text": "[] array of TOML table (default nil)  When script is given monstache will pass the mongodb document into the script before indexing into elasticsearch.  See the section  Middleware \nfor more information.", 
            "title": "script"
        }, 
        {
            "location": "/config/#namespace_1", 
            "text": "string (default \"\")  The MongoDB namespace, db.collection, to apply the script to", 
            "title": "namespace"
        }, 
        {
            "location": "/config/#routing", 
            "text": "boolean (default false)  Set routing to true if you override the index, routing or parent metadata via _meta_monstache", 
            "title": "routing"
        }, 
        {
            "location": "/config/#script_2", 
            "text": "string (default \"\")  An inline script.  You can use TOML multiline syntax here", 
            "title": "script"
        }, 
        {
            "location": "/config/#path_1", 
            "text": "string (default \"\")  The file path to load a script from.  Use this or an inline script but not both. Can be a\npath relative to the directory monstache is executed from or an absolute path.", 
            "title": "path"
        }, 
        {
            "location": "/config/#graylog-addr", 
            "text": "string (default \"\")  The address of a graylog server to redirect logs to in GELF", 
            "title": "graylog-addr"
        }, 
        {
            "location": "/config/#logs", 
            "text": "TOML table (default nil)  Allows writing logs to a file using a rolling appender instead of stdout.  Supply a file path for each type of log you would like to send to a file.", 
            "title": "logs"
        }, 
        {
            "location": "/config/#info", 
            "text": "string (default \"\")  The file path to write info level logs to", 
            "title": "info"
        }, 
        {
            "location": "/config/#error", 
            "text": "string (default \"\")  The file path to write error level logs to", 
            "title": "error"
        }, 
        {
            "location": "/config/#trace", 
            "text": "string (default \"\")  The file path to write trace level logs to. Trace logs are enabled via the verbose option.", 
            "title": "trace"
        }, 
        {
            "location": "/config/#stats_1", 
            "text": "string (default \"\")  The file path to write indexing statistics to. Stats logs are enabled via the stats option.", 
            "title": "stats"
        }, 
        {
            "location": "/config/#enable-http-server", 
            "text": "boolean (default false)  Add this flag to enable an embedded HTTP server at localhost:8080", 
            "title": "enable-http-server"
        }, 
        {
            "location": "/config/#http-server-addr", 
            "text": "string (default \":8080\")  The address to bind the embedded HTTP server on if enabled", 
            "title": "http-server-addr"
        }, 
        {
            "location": "/advanced/", 
            "text": "Advanced\n\n\n\n\nGridFS Support\n\n\nMonstache supports indexing the raw content of files stored in GridFS into Elasticsearch for full\ntext search.  This feature requires that you install an Elasticsearch plugin which enables the field type \nattachment\n.\nFor versions of Elasticsearch prior to version 5 you should install the \n\nmapper-attachments\n plugin.\nFor version 5 or later of Elasticsearch you should instead install the \n\ningest-attachment\n plugin.\n\n\nOnce you have installed the appropriate plugin for Elasticsearch, getting file content from GridFS into Elasticsearch is\nas simple as configuring monstache.  You will want to enable the \nindex-files\n option and also tell monstache the \nnamespace of all collections which will hold GridFS files. For example in your TOML config file,\n\n\nindex-files = true\n\ndirect-read-namespaces = [\nusers.fs.files\n, \nposts.fs.files\n]\n\nfile-namespaces = [\nusers.fs.files\n, \nposts.fs.files\n]\n\nfile-highlighting = true\n\n\n\n\nThe above configuration tells monstache that you wish to index the raw content of GridFS files in the \nusers\n and \nposts\n\nMongoDB databases. By default, MongoDB uses a bucket named \nfs\n, so if you just use the defaults your collection name will\nbe \nfs.files\n.  However, if you have customized the bucket name, then your file collection would be something like \nmybucket.files\n\nand the entire namespace would be \nusers.mybucket.files\n.\n\n\nWhen you configure monstache this way it will perform an additional operation at startup to ensure the destination indexes in\nElasticsearch have a field named \nfile\n with a type mapping of \nattachment\n.  \n\n\nFor the example TOML configuration above, monstache would initialize 2 indices in preparation for indexing into\nElasticsearch by issuing the following REST commands:\n\n\nFor Elasticsearch versions prior to version 5...\n\n\nPOST /users.fs.files\n{\n  \"mappings\": {\n    \"fs.files\": {\n      \"properties\": {\n    \"file\": { \"type\": \"attachment\" }\n}}}}\n\nPOST /posts.fs.files\n{\n  \"mappings\": {\n    \"fs.files\": {\n      \"properties\": {\n    \"file\": { \"type\": \"attachment\" }\n}}}}\n\n\n\nFor Elasticsearch version 5 and above...\n\n\nPUT /_ingest/pipeline/attachment\n{\n  \"description\" : \"Extract file information\",\n  \"processors\" : [\n    {\n      \"attachment\" : {\n    \"field\" : \"file\"\n      }\n    }\n  ]\n}\n\n\n\nWhen a file is inserted into MongoDB via GridFS, monstache will detect the new file, use the MongoDB api to retrieve the raw\ncontent, and index a document into Elasticsearch with the raw content stored in a \nfile\n field as a base64 \nencoded string. The Elasticsearch plugin will then extract text content from the raw content using \n\nApache Tika\n, tokenize the text content, and allow you to query on the content of the file.\n\n\nTo test this feature of monstache you can simply use the \nmongofiles\n\ncommand to quickly add a file to MongoDB via GridFS.  Continuing the example above one could issue the following command to put a \nfile named \nresume.docx\n into GridFS and after a short time this file should be searchable in Elasticsearch in the index \nusers.fs.files\n.\n\n\nmongofiles -d users put resume.docx\n\n\n\nAfter a short time you should be able to query the contents of resume.docx in the users index in Elasticsearch\n\n\ncurl -XGET \"http://localhost:9200/users.fs.files/_search?q=golang\"\n\n\n\nIf you would like to see the text extracted by Apache Tika you can project the appropriate sub-field\n\n\nFor Elasticsearch versions prior to version 5...\n\n\ncurl -H \"Content-Type:application/json\" localhost:9200/users.fs.files/_search?pretty -d '{\n    \"fields\": [ \"file.content\" ],\n    \"query\": {\n        \"match\": {\n            \"file.content\": \"golang\"\n        }\n    }\n}'\n\n\n\nFor Elasticsearch version 5 and above...\n\n\ncurl -H \"Content-Type:application/json\" localhost:9200/users.fs.files/_search?pretty -d '{\n    \"_source\": [ \"attachment.content\" ],\n    \"query\": {\n        \"match\": {\n            \"attachment.content\": \"golang\"\n        }\n    }\n}'\n\n\n\nWhen \nfile-highlighting\n is enabled you can add a highlight clause to your query\n\n\nFor Elasticsearch versions prior to version 5...\n\n\ncurl -H \"Content-Type:application/json\" localhost:9200/users.fs.files/_search?pretty -d '{\n    \"fields\": [\"file.content\"],\n    \"query\": {\n        \"match\": {\n            \"file.content\": \"golang\"\n        }\n    },\n    \"highlight\": {\n        \"fields\": {\n            \"file.content\": {\n            }\n        }\n    }\n}'\n\n\n\nFor Elasticsearch version 5 and above...\n\n\ncurl -H \"Content-Type:application/json\" localhost:9200/users.fs.files/_search?pretty -d '{\n    \"_source\": [\"attachment.content\"],\n    \"query\": {\n        \"match\": {\n            \"attachment.content\": \"golang\"\n        }\n    },\n    \"highlight\": {\n        \"fields\": {\n            \"attachment.content\": {\n            }\n        }\n    }\n}'\n\n\n\nThe highlight response will contain emphasis on the matching terms\n\n\nFor Elasticsearch versions prior to version 5...\n\n\n\"hits\" : [ {\n    \"highlight\" : {\n        \"file.content\" : [ \"I like to program in \nem\ngolang\n/em\n.\\n\\n\" ]\n    }\n} ]\n\n\n\nFor Elasticsearch version 5 and above...\n\n\n\"hits\" : [{\n    \"highlight\" : {\n        \"attachment.content\" : [ \"I like to program in \nem\ngolang\n/em\n.\" ]\n    }\n}]\n\n\n\nWorkers\n\n\nYou can run multiple monstache processes and distribute the work between them.  First configure\nthe names of all the workers in a shared config.toml file.\n\n\nworkers = [\nTom\n, \nDick\n, \nHarry\n]\n\n\n\n\nIn this case we have 3 workers.  Now we can start 3 monstache processes and give each one of the worker\nnames.\n\n\nmonstache -f config.toml -worker Tom\nmonstache -f config.toml -worker Dick\nmonstache -f config.toml -worker Harry\n\n\n\nmonstache will hash the id of each document using consistent hashing so that each id is handled by only\none of the available workers.\n\n\nHigh Availability\n\n\nYou can run monstache in high availability mode by starting multiple processes with the same value for \ncluster-name\n.\nEach process will join a cluster which works together to ensure that a monstache process is always syncing to Elasticsearch.\n\n\nHigh availability works by ensuring one active process in the \nmonstache.cluster\n collection in mongodb at any given time. Only the process in\nthis collection will be syncing for the cluster.  Processes not present in this collection will be paused.  Documents in the \n\nmonstache.cluster\n collection have a TTL assigned to them.  When a document in this collection times out it will be removed from\nthe collection by mongodb and another process in the monstache cluster will have a chance to write to the collection and become the\nnew active process.\n\n\nWhen \ncluster-name\n is supplied the \nresume\n feature is automatically turned on and the \nresume-name\n becomes the name of the cluster.\nThis is to ensure that each of the processes is able to pick up syncing where the last one left off.  \n\n\nYou can combine the HA feature with the workers feature.  For 3 cluster nodes with 3 workers per node you would have something like the following:\n\n\n// config.toml\nworkers = [\"Tom\", \"Dick\", \"Harry\"]\n\n// on host A\nmonstache -cluster-name HA -worker Tom -f config.toml\nmonstache -cluster-name HA -worker Dick -f config.toml\nmonstache -cluster-name HA -worker Harry -f config.toml\n\n// on host B\nmonstache -cluster-name HA -worker Tom -f config.toml\nmonstache -cluster-name HA -worker Dick -f config.toml\nmonstache -cluster-name HA -worker Harry -f config.toml\n\n// on host C\nmonstache -cluster-name HA -worker Tom -f config.toml\nmonstache -cluster-name HA -worker Dick -f config.toml\nmonstache -cluster-name HA -worker Harry -f config.toml\n\n\n\nWhen the clustering feature is combined with workers then the \nresume-name\n becomes the cluster name concatenated with the worker name.\n\n\nIndex Mapping\n\n\nWhen indexing documents from MongoDB into elasticsearch the default mapping is as follows:\n\n\nFor Elasticsearch prior to 6.2\n\n\nelasticsearch index name    \n= mongodb database name . mongodb collection name\nelasticsearch type          \n= mongodb collection name\nelasticsearch document _id  \n= mongodb document _id\n\n\n\n\nFor Elasticsearch 6.2+\n\n\nelasticsearch index name    \n= mongodb database name . mongodb collection name\nelasticsearch type          \n= _doc \nelasticsearch document _id  \n= mongodb document _id\n\n\n\n\nIf these default won't work for some reason you can override the index and type mapping on a per collection basis by adding\nthe following to your TOML config file:\n\n\n[[mapping]]\nnamespace = \ntest.test\n\nindex = \nindex1\n\ntype = \ntype1\n\n\n[[mapping]]\nnamespace = \ntest.test2\n\nindex = \nindex2\n\ntype = \ntype2\n\n\n\n\n\nWith the configuration above documents in the \ntest.test\n namespace in MongoDB are indexed into the \nindex1\n \nindex in elasticsearch with the \ntype1\n type.\n\n\nIf you need your index and type mapping to be more dynamic, such as based on values inside the MongoDB document, then\nsee the sections \nMiddleware\n and  \nRouting\n.\n\n\n\n\nWarning\n\n\nIt is not recommended to override the default type of \n_doc\n if using Elasticsearch 6.2+ since this will be the supported path going forward.\nAlso, using \n_doc\n as the type will not work with Elasticsearch prior to 6.2.\n\n\n\n\nMake sure that automatic index creation is not disabled in elasticsearch.yml or create your target indexes before using Monstache.\n\n\nIf automatic index creation must be controlled, whitelist any indexes in elasticsearch.yml that monstache will create.\n\n\nNamespaces\n\n\nWhen a document is inserted, updated, or deleted in mongodb a document is appended to the oplog representing the event.  This document has a field \nns\n which is the namespace.  For inserts, updates, and deletes the namespace is the database name and collection name of the document changed joined by a dot. E.g. for \nuse test; db.foo.insert({hello: \"world\"});\n the namespace for the event in the oplog would be \ntest.foo\n.\n\n\nIn addition to inserts, updates, and deletes monstache also supports database and collection drops.  When a database or collection is dropped in mongodb an event is appended to the oplog.  Like the other types of changes this event has a field \nns\n representing the namespace.  However, for drops the namespace is the database name and the string \n$cmd\n joined by a dot.  E.g. for \nuse test; db.foo.drop()\n the namespace for the event in the oplog would be \ntest.$cmd\n.  \n\n\nWhen configuring namespaces in monstache you will need to account for both cases.  \n\n\n\n\nWarning\n\n\nBe careful if you have configured \ndropped-databases=true\n or \ndropped-collections=true\n AND you also have a \nnamespace-regex\n set.  If your namespace regex does not take into account the \ndb.$cmd\n namespace the event may be filtered and the elasticsearch index not deleted on a drop.\n\n\n\n\nMiddleware\n\n\nmonstache supports embedding user defined middleware between MongoDB and Elasticsearch.  middleware is able to transform documents,\ndrop documents, or define indexing metadata.  middleware may be written in either Javascript or in Golang as a plugin.  Golang plugins\nrequire Go version 1.8 or greater on Linux. currently, you are able to use Javascript or Golang but not both (this may change in the future).\n\n\nGolang\n\n\nmonstache supports Golang 1.8+ plugins on Linux.  To implement a plugin for monstache you simply need to implement a specific function signature,\nuse the go command to build a .so file for your plugin, and finally pass the path to your plugin .so file when running monstache.\n\n\nTo create a golang plugin for monstache\n\n\n\n\nget the necessary dependencies with \ngo get -u github.com/rwynn/monstache/monstachemap\n\n\ncreate a .go source file that belongs to the package \nmain\n\n\nimport \ngithub.com/rwynn/monstache/monstachemap\n\n\nimplement a function named \nMap\n with the following signature\n\n\n\n\nfunc Map(input *monstachemap.MapperPluginInput) (output *monstachemap.MapperPluginOutput, err error)\n\n\n\n\n\n\noptionally implement a function named \nFilter\n with the following signature\n\n\n\n\nfunc Filter(input *monstachemap.MapperPluginInput) (keep bool, err error)\n\n\n\n\nplugins can be compiled using\n\n\ngo build -buildmode=plugin -o myplugin.so myplugin.go\n\n\n\nto enable the plugin, start monstache with\n\n\nmonstache -mapper-plugin-path /path/to/myplugin.so\n\n\n\nthe following example plugin simply converts top level string values to uppercase\n\n\npackage main\nimport (\n    \ngithub.com/rwynn/monstache/monstachemap\n\n    \nstrings\n\n)\n// a plugin to convert document values to uppercase\nfunc Map(input *monstachemap.MapperPluginInput) (output *monstachemap.MapperPluginOutput, err error) {\n    doc := input.Document\n    for k, v := range doc {\n        switch v.(type) {\n        case string:\n            doc[k] = strings.ToUpper(v.(string))\n        }\n    }\n    output = \nmonstachemap.MapperPluginOutput{Document: doc}\n    return\n}\n\n\n\n\nThe input parameter will contain information about the document's origin database and collection.\n\n\nTo drop the document (direct monstache not to index it) set \noutput.Drop = true\n.\n\n\nTo simply pass the original document through to Elasticsearch, set \noutput.Passthrough = true\n\n\nTo set custom indexing metadata on the document use \noutput.Index\n, \noutput.Type\n, \noutput.Parent\n and \noutput.Routing\n.\n\n\n\n\nNote\n\n\nIf you override \noutput.Parent\n or \noutput.Routing\n for any MongoDB namespaces in a \ngolang plugin you should also add those namespaces to the \nrouting-namespaces\n array \nin your config file.\n\n\nThis instructs Monstache to save the routing information so that deletes of the document work\ncorrectly.\n\n\n\n\nIf would like to embed other MongoDB documents (possibly from a different collection) within the current document \nbefore indexing, you can access the \n*mgo.Session\n pointer as \ninput.Session\n.  With the mgo session you can use the \nmgo API\n to find documents in MongoDB and embed them in the Document set on output.\n\n\nWhen you implement a \nFilter\n function the function is called immediately after reading inserts and updates from the oplog.  You can return false from this function to completely ignore a document.  This is different than setting \noutput.Drop\n from the mapping function because when you set \noutput.Drop\n to true, a delete request is issued to Elasticsearch in case the document had previously been indexed.  By contrast, returning false from the \nFilter\n function causes the operation to be completely ignored and there is no corresponding delete request issued to Elasticsearch.\n\n\nJavascript\n\n\nTransformation\n\n\nMonstache uses the amazing \notto\n library to provide transformation at the document field\nlevel in Javascript.  You can associate one javascript mapping function per mongodb collection.  You can also associate a function \nat the global level by not specifying a namespace.  These javascript functions are\nadded to your TOML config file, for example:\n\n\n[[script]]\nnamespace = \nmydb.mycollection\n\nscript = \n\nvar counter = 1;\nmodule.exports = function(doc) {\n    doc.foo += \ntest\n + counter;\n    counter++;\n    return doc;\n}\n\n\n\n[[script]]\nnamespace = \nanotherdb.anothercollection\n\npath = \npath/to/transform.js\n\nrouting = true\n\n[[script]]\n# this script does not declare a namespace\n# it is global to all collections\nscript = \n\nmodule.exports = function(doc, ns) {\n    // the doc namespace e.g. test.test is passed as the 2nd arg\n    return _.omit(doc, \npassword\n, \nsecret\n);\n}\n\n\n\n\n\n\nThe example TOML above configures 3 scripts. The first is applied to \nmycollection\n in \nmydb\n while the second is applied\nto \nanothercollection\n in \nanotherdb\n. The first script is inlined while the second is loaded from a file path.  The path can be absolute or relative to the directory monstache is executed from.\nThe last script does not specify a namespace, so documents from all collections pass through it. Global scripts are run before scripts which are\nlinked to a specific namespace.\n\n\nYou will notice that the multi-line string feature of TOML is used to assign a javascript snippet to the variable named\n\nscript\n.  The javascript assigned to script must assign a function to the exports property of the \nmodule\n object.  This \nfunction will be passed the document from mongodb just before it is indexed in elasticsearch.  Inside the function you can\nmanipulate the document to drop fields, add fields, or augment the existing fields.\n\n\nThe \nthis\n reference in the mapping function is assigned to the document from mongodb.  \n\n\nWhen the return value from the mapping function is an \nobject\n then that mapped object is what actually gets indexed in elasticsearch.\nFor these purposes an object is a javascript non-primitive, excluding \nFunction\n, \nArray\n, \nString\n, \nNumber\n, \nBoolean\n, \nDate\n, \nError\n and \nRegExp\n.\n\n\nFiltering\n\n\nYou can completely ignore documents by adding filter configurations to your TOML config file.  The filter functions are executing immediately after inserts or updates are read from the oplog.  The correspding document is passed into the function and you can return true or false to include or ignore the document.\n\n\n[[filter]]\nnamespace = \ndb.collection\n\nscript = \n\nmodule.exports = function(doc) {\n    return !!doc.interesting;\n}\n\n\n\n[[filter]]\nnamespace = \ndb2.collection2\n\npath = \npath/to/script.js\n\n\n\n\n\nDropping\n\n\nIf the return value from the mapping function is not an \nobject\n per the definition above then the result is converted into a \nboolean\n\nand if the boolean value is \nfalse\n then that indicates to monstache that you would not like to index the document. If the boolean value is \ntrue\n then\nthe original document from mongodb gets indexed in elasticsearch.\n\n\nThis allows you to return false or null if you have implemented soft deletes in mongodb.\n\n\n[[script]]\nnamespace = \ndb.collection\n\nscript = \n\nmodule.exports = function(doc) {\n    if (!!doc.deletedAt) {\n        return false;\n    }\n    return true;\n}\n\n\n\n\n\n\nIn the above example monstache will index any document except the ones with a \ndeletedAt\n property.  If the document is first\ninserted without a \ndeletedAt\n property, but later updated to include the \ndeletedAt\n property then monstache will remove, or drop, the\npreviously indexed document from the elasticsearch index. \n\n\n\n\nNote\n\n\nDropping a document is different that filtering a document.  A filtered document is completely ignored.  A dropped document results in a delete request being issued to Elasticsearch in case the document had previously been indexed.\n\n\n\n\nScripting Features\n\n\nYou may have noticed that in the first example above the exported mapping function closes over a var named \ncounter\n.  You can\nuse closures to maintain state between invocations of your mapping function.\n\n\nFinally, since Otto makes it so easy, the venerable \nUnderscore\n library is included for you at \nno extra charge.  Feel free to abuse the power of the \n_\n.\n\n\nEmbedding Documents\n\n\nIn your javascript function you have access to the following global functions to retreive documents from MongoDB for\nembedding in the current document before indexing.  Using this approach you can pull in related data.\n\n\nfunction findId(documentId, [options]) {\n    // convenience method for findOne({_id: documentId})\n    // returns 1 document or null\n}\n\nfunction findOne(query, [options]) {\n    // returns 1 document or null\n}\n\nfunction find(query, [options]) {\n    // returns an array of documents or null\n}\n\n\n\n\nEach function takes a \nquery\n object parameter and an optional \noptions\n object parameter.\n\n\nThe options object takes the following keys and values:\n\n\nvar options = {\n    database: \ntest\n,\n    collection: \ntest\n,\n    // to omit _id set the _id key to 0 in select\n    select: {\n        age: 1\n    },\n    // only applicable to find...\n    sort: [\nname\n],\n    limit: 2\n}\n\n\n\n\nIf the database or collection keys are omitted from the options object, the values for database and/or\ncollection are set to the database and collection of the document being processed.\n\n\nHere are some examples:\n\n\nThis example sorts the documents in the same collection as the document being processed by name and returns\nthe first 2 documents projecting only the age field.  The result is set on the current document before being\nindexed.\n\n\n[[script]]\nnamespace = \ntest.test\n\nscript = \n\nmodule.exports = function(doc) {\n    doc.twoAgesSortedByName = find({}, {\n            sort: [\nname\n],\n            limit: 2,\n            select: {\n              age: 1\n            }\n    });\n    return doc;\n}\n\n\n\n\n\n\nThis example grabs a reference id from a document and replaces it with the corresponding document with that id.\n\n\n[[script]]\nnamespace = \ntest.posts\n\nscript = \n\nmodule.exports = function(post) {\n    if (post.author) { // author is a an object id reference\n        post.author = findId(post.author, {\n          database: \ntest\n,\n          collection: \nusers\n\n        });\n    }\n    return post;\n}\n\n\n\n\n\n\nIndexing Metadata\n\n\nYou can override the indexing metadata for an individual document by setting a special field named\n\n_meta_monstache\n on the document you return from your Javascript function.\n\n\nAssume there is a collection in MongoDB named \ncompany\n in the \ntest\n database.\nThe documents in this collection look like either\n\n\n{ \n_id\n: \nlondon\n, \ntype\n: \nbranch\n, \nname\n: \nLondon Westminster\n, \ncity\n: \nLondon\n, \ncountry\n: \nUK\n }\n\n\n\n\nor\n\n\n{ \n_id\n: \nalice\n, \ntype\n: \nemployee\n, \nname\n: \nAlice Smith\n, \nbranch\n: \nlondon\n }\n\n\n\n\nGiven the above the following snippet sets up a parent-child relationship in Elasticsearch based on the\nincoming documents from MongoDB and updates the ns (namespace) from test.company to company in elasticsearch\n\n\n[[script]]\nnamespace = \ntest.company\n\nrouting = true\nscript = \n\nmodule.exports = function(doc, ns) {\n        // var meta = { type: doc.type, index: 'company' };\n    var meta = { type: doc.type, index: ns.split(\n.\n)[1] };\n    if (doc.type === \nemployee\n) {\n        meta.parent = doc.branch;\n    }\n    doc._meta_monstache = meta;\n    return _.omit(doc, \nbranch\n, \ntype\n);\n}\n\n\n\n\n\n\nThe snippet above will route these documents to the \ncompany\n index in Elasticsearch instead of the\ndefault of \ntest.company\n, if you didn't specify a namespace, it'll route all documents to indexes named as the collection only without the database \ndb\n.\ncollection\n (mongodb) =\n \ncollection\n (elasticsearch).  Also, instead of using \ncompany\n as the Elasticsearch type, the type\nattribute from the document will be used as the Elasticsearch type.  Finally, if the type is\nemployee then the document will be indexed as a child of the branch the person belongs to.\n\n\nWe can throw away the type and branch information by deleting it from the document before returning\nsince the type information will be stored in Elasticsearch under \n_type\n and the branch information\nwill be stored under \n_parent\n.\n\n\nThe example is based on the Elasticsearch docs for \nparent-child\n\n\nFor more on updating the namespace name, check the \nDelete Strategy\n as there is a change of the default behviour in v4.4.0 \n v3.11.0\n\n\nRouting\n\n\nRouting is the process by which Elasticsearch determines which shard a document will reside in.  Monstache\nsupports user defined, or custom, routing of your MongoDB documents into Elasticsearch.  \n\n\nConsider an example where you have a \ncomments\n collection in MongoDB which stores a comment and \nits associated post identifier.  \n\n\nuse blog;\ndb.comments.insert({title: \nDid you read this?\n, post_id: \n123\n});\ndb.comments.insert({title: \nYeah, it's good\n, post_id: \n123\n});\n\n\n\n\nIn this case monstache will index those 2 documents in an index named \nblog.comments\n under the id\ncreated by MongoDB.  When Elasticsearch routes a document to a shard, by default, it does so by hashing \nthe id of the document.  This means that as the number of comments on post \n123\n grows, each of the comments\nwill be distributed somewhat evenly between the available shards in the cluster.  \n\n\nThus, when a query is performed searching among the comments for post \n123\n Elasticsearch will need to query\nall of those shards just in case a comment happened to have been routed there.\n\n\nWe can take advantage of the support in Elasticsearch and in monstache to do some intelligent\nrouting such that all comments for post \n123\n reside in the same shard.\n\n\nFirst we need to tell monstache that we would like to do custom routing for this collection by setting \nrouting\n\nequal to true on a custom script for the namespace.  Then we need to add some metadata to the document telling\nmonstache how to route the document when indexing.  In this case we want to route by the \npost_id\n field.\n\n\n[[script]]\nnamespace = \nblog.comments\n\nrouting = true\nscript = \n\nmodule.exports = function(doc) {\n    doc._meta_monstache = { routing: doc.post_id };\n    return doc;\n}\n\n\n\n\n\n\nNow when monstache indexes document for the collection \nblog.comments\n it will set the special \n_routing\n attribute\nfor the document on the index request such that Elasticsearch routes comments based on their corresponding post. \n\n\nThe \n_meta_monstache\n field is used only to inform monstache about routing and is not included in the source\ndocument when indexing to Elasticsearch.  \n\n\nNow when we are searching for comments and we know the post id that the comment belongs to we can include that post\nid in the request and make a search that normally queries all shards query only 1 shard.\n\n\n$ curl -H \nContent-Type:application/json\n -XGET 'http://localhost:9200/blog.comments/_search?routing=123' -d '\n{\n   \nquery\n:{\n      \nmatch_all\n:{}\n   }\n}'\n\n\n\n\nYou will notice in the response that only 1 shard was queried instead of all your shards.  Custom routing is great\nway to reduce broadcast searches and thus get better performance.\n\n\nThe catch with custom routing is that you need to include the routing parameter on all insert, update, and delete\noperations.  Insert and update is not a problem for monstache because the routing information will come from your\nMongoDB document.  Deletes, however, pose a problem for monstache because when a delete occurs in MongoDB the\ninformation in the oplog is limited to the id of the document that was deleted.  But monstache needs to know where the\ndocument was originally routed in order to tell Elasticsearch where to look for it.\n\n\nMonstache has 3 available strategies for handling deletes in this situation.  The default strategy is stateless and uses\na term query into Elasticsearch based on the ID of the document deleted in MongoDB.  If the search into Elasticsearch returns\nexactly 1 document then monstache will schedule that document for deletion. The 2nd stategy monstache uses is stateful and requires\ngiving monstache the ability to write to the collection \nmonstache.meta\n.  In this collection monstache stores information about\ndocuments that were given custom indexing metadata.  This stategy slows down indexing and takes up space in MongoDB.\n\nHowever, it is precise because it records exactly how each document was indexed. The final stategy simply punts on deletes and\nleaves document deletion to the user.  If you don't generally delete documents in MongoDB or don't care if Elasticsearch contains\ndocuments which have been deleted in MongoDB, this option is available. See \nDelete Strategy\n for more information.\n\n\nFor more information see \nCustomizing Document Routing\n\n\nIn addition to letting your customize the shard routing for a specific document, you can also customize the elasticsearch\n\nindex\n and \ntype\n using a script by putting the custom information in the meta attribute. \n\n\n[[script]]\nnamespace = \nblog.comments\n\nrouting = true\nscript = \n\nmodule.exports = function(doc) {\n    if (doc.score \n= 100) {\n        // NOTE: prefix dynamic index with namespace for proper cleanup on drops\n        doc._meta_monstache = { index: \nblog.comments.highscore\n, type: \nhighScoreComment\n, routing: doc.post_id };\n    } else {\n        doc._meta_monstache = { routing: doc.post_id };\n    }\n    return doc;\n}\n\n\n\n\n\n\nJoins\n\n\nElasticsearch 6 introduces an updated approach to parent-child called joins.  The following example shows how you can accomplish joins\nwith Monstache.  The example is based on the Elasticsearch \ndocumentation\n.\n\n\nThis example assumes Monstache is syncing the \ntest.test\n collection in MongoDB with the \ntest.test\n index in Elasticsearch.\n\n\nFirst we will want to setup an index mapping in Elasticsearch describing the join field.  \n\n\ncurl -XPUT 'localhost:9200/test.test?pretty' -H 'Content-Type: application/json' -d'\n{\n  \nmappings\n: {\n    \n_doc\n: {\n      \nproperties\n: {\n        \nmy_join_field\n: { \n          \ntype\n: \njoin\n,\n          \nrelations\n: {\n            \nquestion\n: \nanswer\n \n          }\n        }\n      }\n    }\n  }\n}\n'\n\n\n\n\n\n\nWarning\n\n\nThe above mapping uses _doc as the Elasticsearch type. _doc is the recommended type for new\nversions Elasticsearch but it only works with Elasticsearch versions 6.2 and greater.  Monstache\ndefaults to using _doc as the type when it detects Elasticsearch version 6.2 or greater.  If you\nare using a previous version of Elasticsearch monstache defaults to using the MongoDB collection\nname as the Elasticsearch type. The type Monstache uses can be overriden but it is not\nrecommended from Elasticsearch 6.2 on. \n\n\n\n\nNext will will configure Monstache with custom Javascript middleware that does transformation and routing.  In a file called CONFIG.toml.\n\n\n[[script]]\nnamespace = \ntest.test\n\nrouting = true\nscript = \n\nmodule.exports = function(doc) {\n        var routing;\n        if (doc.type === \nquestion\n) {\n                routing = doc._id;\n                doc.my_join_field = {\n                   name: \nquestion\n\n                }\n        } else if (doc.type === \nanswer\n) {\n                routing = doc.question;\n                doc.my_join_field = {\n                  name: \nanswer\n,\n                  parent: routing\n                };\n        }\n        if (routing) {\n                doc._meta_monstache = { routing: routing };\n        }\n        return doc;\n}\n\n\n\n\n\n\nThe mapping function adds a \nmy_join_field\n field to each document.  The contents of the field are based on the \ntype\n attribute in the MongoDB\ndocument. Also, the function ensures that the routing is always based on the _id of the question document.   \n\n\nNow with this config in place we can start Monstache.  We will use verbose to see the requests.\n\n\nmonstache -verbose -f CONFIG.toml\n\n\n\n\nWith Monstache running we are now ready to insert into MongoDB\n\n\n\nrs:PRIMARY\n use test;\nswitched to db test\n\nrs:PRIMARY\n db.test.insert({type: \nquestion\n, text: \nThis is a question\n});\n\nrs:PRIMARY\n db.test.find()\n{ \n_id\n : ObjectId(\n5a84a8b826993bde57c12893\n), \ntype\n : \nquestion\n, \ntext\n : \nThis is a question\n }\n\nrs:PRIMARY\n db.test.insert({type: \nanswer\n, text: \nThis is an answer\n, question: ObjectId(\n5a84a8b826993bde57c12893\n) });\n\n\n\n\n\nWhen we insert these documents we should see Monstache generate the following requests to Elasticsearch\n\n\n\n{\nindex\n:{\n_id\n:\n5a84a8b826993bde57c12893\n,\n_index\n:\ntest.test\n,\n_type\n:\n_doc\n,\nrouting\n:\n5a84a8b826993bde57c12893\n,\nversion\n:6522523668566769665,\nversion_type\n:\nexternal\n}}\n{\nmy_join_field\n:{\nname\n:\nquestion\n},\ntext\n:\nThis is a question\n,\ntype\n:\nquestion\n}\n\n{\nindex\n:{\n_id\n:\n5a84a92b26993bde57c12894\n,\n_index\n:\ntest.test\n,\n_type\n:\n_doc\n,\nrouting\n:\n5a84a8b826993bde57c12893\n,\nversion\n:6522524162488008705,\nversion_type\n:\nexternal\n}}\n{\nmy_join_field\n:{\nname\n:\nanswer\n,\nparent\n:\n5a84a8b826993bde57c12893\n},\nquestion\n:\n5a84a8b826993bde57c12893\n,\ntext\n:\nThis is an answer\n,\ntype\n:\nanswer\n}\n\n\n\n\n\nThis looks good.  We should now have a parent/child relationship between these documents in Elasticsearch.\n\n\nIf we do a search on the \ntest.test\n index we see the following results:\n\n\n\n \nhits\n : {\n    \ntotal\n : 2,\n    \nmax_score\n : 1.0,\n    \nhits\n : [\n      {\n        \n_index\n : \ntest.test\n,\n        \n_type\n : \n_doc\n,\n        \n_id\n : \n5a84a8b826993bde57c12893\n,\n        \n_score\n : 1.0,\n        \n_routing\n : \n5a84a8b826993bde57c12893\n,\n        \n_source\n : {\n          \nmy_join_field\n : {\n            \nname\n : \nquestion\n\n          },\n          \ntext\n : \nThis is a question\n,\n          \ntype\n : \nquestion\n\n        }\n      },\n      {\n        \n_index\n : \ntest.test\n,\n        \n_type\n : \n_doc\n,\n        \n_id\n : \n5a84a92b26993bde57c12894\n,\n        \n_score\n : 1.0,\n        \n_routing\n : \n5a84a8b826993bde57c12893\n,\n        \n_source\n : {\n          \nmy_join_field\n : {\n            \nname\n : \nanswer\n,\n            \nparent\n : \n5a84a8b826993bde57c12893\n\n          },\n          \nquestion\n : \n5a84a8b826993bde57c12893\n,\n          \ntext\n : \nThis is an answer\n,\n          \ntype\n : \nanswer\n\n        }\n      }\n    ]\n  }\n\n\n\n\n\nTo clean up our documents in Elasticsearch a bit we can omit the information that we don't really need in the source docs by \nupdating our mapping function. This information needs not be at the top-level since it is duplicated in \nmy_join_field\n.\n\n\n    return _.omit(doc, \ntype\n, \nquestion\n);\n\n\n\n\n\nIf your parent and child documents are in separate MongoDB collections then you would set up a script\nfor each collection.  You can tell if the doc is a parent or child by the collection it comes from. The only other difference would be that you would need to override the index dynamically in addition to the routing such that documents from both MongoDB collections target the same \nindex.\n\n\n    doc._meta_monstache = { routing: routing, index: \nparentsAndChildren\n };\n\n\n\n\n\n\nWarning\n\n\nYou must be careful when you route 2 or more MongoDB collections to the same Elasticsearch index\nthat the document _ids across the MongoDB collections do not collide for any 2 docs because \nthey will be used as the _id in the target index. \n\n\n\n\nTime Machines\n\n\nIf you are not just interested in what the current value of a document in MongoDB is, but also would like to see how it has changed over time use \ntime machine namespaces\n.   For example, you've inserted and later updated a document with id 123 in the \ntest.test\n collection in MongoDB. If \ntest.test\n is a time machine namespace you will have 2 documents representing those changes in the \nlog.test.test.2018-02-20\n index (timestamp will change) in Elasticsearch.  If you later want all the changes made to that document in MongoDB you can issue a query like this:\n\n\n$ curl -XGET 'http://localhost:9200/log.test.test.*/_search?routing=123' -d '\n{\n   \nquery\n:{\n      \nsort\n : [\n        { \n_oplog_ts\n : {\norder\n : \ndesc\n}}\n      ],\n      \nfiltered\n:{\n         \nquery\n:{\n            \nmatch_all\n:{}\n         },\n         \nfilter\n:{\n            \nterm\n:{\n               \n_source_id\n:\n123\n\n            }\n         }\n      }\n   }\n}'\n\n\n\n\nThat query will be very efficient because it only queries the shard that all the change docs went to for MongoDB document id 123.  It filters the documents on that shard by \n_source_id\n, or id from MongoDB, to only give us the changes to that document.  Finally, it sorts by the \n_oplog_ts\n which gives us the most recent change docs first.   \n\n\nThe index pattern in the query is a wildcard to pick up all the timestamped indexes that we've acculated for the \ntest.test\n namespace.\n\n\nMerge Patches\n\n\nA unique feature of monstache is support for JSON Merge Patches \nrfc-7396\n.\n\n\nIf merge patches are enabled monstache will add an additional field to documents indexed into elasticsearch. The\nname of this field is configurable but it defaults to \njson-merge-patches\n.  \n\n\nConsider the following example with merge patches enabled... \n\n\ndb.test.insert({name: \nJoe\n, age: 16, friends: [1, 2, 3]})\n\n\n\n\nAt this point you would have the following document source in elasticsearch.\n\n\n \"_source\" : {\n  \"age\" : 16,\n  \"friends\" : [\n    1,\n    2,\n    3\n  ],\n  \"json-merge-patches\" : [\n    {\n      \"p\" : \"{\\\"age\\\":16,\\\"friends\\\":[1,2,3],\\\"name\\\":\\\"Joe\\\"}\",\n      \"ts\" : 1487263414,\n      \"v\" : 1\n    }\n  ],\n  \"name\" : \"Joe\"\n}\n\n\n\nAs you can see we have a single timestamped merge patch in the json-merge-patches array.  \n\n\nNow let's update the document to remove a friend and update the age.\n\n\ndb.test.update({name: \nJoe\n}, {$set: {age: 21, friends: [1, 3]}})\n\n\n\n\nIf we now look at the document in elasticsearch we see the following:\n\n\n\"_source\" : {\n  \"age\" : 21,\n  \"friends\" : [\n    1,\n    3\n  ],\n  \"json-merge-patches\" : [\n    {\n      \"p\" : \"{\\\"age\\\":16,\\\"friends\\\":[1,2,3],\\\"name\\\":\\\"Joe\\\"}\",\n      \"ts\" : 1487263414,\n      \"v\" : 1\n    },\n    {\n      \"p\" : \"{\\\"age\\\":21,\\\"friends\\\":[1,3]}\",\n      \"ts\" : 1487263746,\n      \"v\" : 2\n    }\n  ],\n  \"name\" : \"Joe\"\n}\n\n\n\nYou can see that the document was updated as expected and an additional merge patch was added.\n\n\nEach time the document is updated in mongodb the corresponding document in elasticsearch gains a\ntimestamped merge patch.  Using this information we can time travel is the document's history.\n\n\nThere is a merge patch for each version of the document.  To recreate a specific version we simply need\nto apply the merge patches in order up to the version that we want.\n\n\nTo get version 1 of the document above we start with {} and apply the 1st merge patch.  \n\n\nTo get version 2 of the document above we start with {}\n\n\n\n\napply the 1st merge patch to get v1\n\n\napply the 2nd merge patch to v1 to get v2\n\n\n\n\nThe timestamps associated with these merge patches are in seconds since the epoch, taken from the\ntimestamp recorded in the oplog when the insert or update occured. \n\n\nTo enable the merge patches feature in monstache you need to add the following to you TOML config:\n\n\nenable-patches = true\npatch-namespaces = [\"test.test\"]\n\n\n\nYou need you add each namespace that you would like to see patches for in the patch-namespaces array.\n\n\nOptionally, you can change the key under which the patches are stored in the source document as follows:\n\n\nmerge-patch-attribute = \"custom-merge-attr\"\n\n\n\nMerge patches will only be recorded for data read from the MongoDB oplog.  Data read using the direct read\nfeature will not be enhanced with merge patches.\n\n\nMost likely, you will want to turn off indexing for the merge patch attribute.  You can do this by creating\nan index template for each patch namespace before running monstache...\n\n\nPUT /_template/test.test\n{\n    \"template\" : \"test.test\",\n    \"mappings\" : {\n    \"test\" : {\n        \"json-merge-patches\" : { \"index\" : false }\n    }\n    }\n}\n\n\n\nSystemd\n\n\nMonstache has support built in for integrating with systemd. The following \nmonstache.service\n is an example \nsystemd configuration.\n\n\n[Unit]\nDescription=monstache sync service\n\n[Service]\nType=notify\nExecStart=/usr/local/bin/monstache\nWatchdogSec=30s\nRestart=on-failure\n\n[Install]\nWantedBy=multi-user.target\n\n\n\nSystem-d unit files are normally saved to \n/lib/systemd/system\n.  \n\n\nAfter saving the monstache.service file you can run \nsystemctl daemon-reload\n to tell systemd to reload \nall unit files. \n\n\nYou can enable the service to start on boot with \nsystemctl enable monstache.service\n and start the service with \nsystemctl start monstache.service\n.\n\n\nWith the configuration above monstache will notify systemd when it has started successfully and then notify \nsystemd repeatedly at half the WatchDog interval to signal liveness.  The configuration above causes systemd\nto restart monstache if it does not start or respond within the WatchdDog interval.\n\n\nDocker\n\n\nThere are Docker images available for Monstache on \nDocker Hub\n\n\nYou can pull and run the latest images with\n\n\n# for elasticsearch \n 5\ndocker run rwynn/monstache:latest -v\n\n# for elasticsearch \n= 5\ndocker run rwynn/monstache:rel3 -v\n\n\n\n\nYou can pull and run release images with\n\n\ndocker run rwynn/monstache:4.6.4 -v\n\ndocker run rwynn/monstache:3.13.4 -v\n\n\n\n\nFor example, to run monstache via Docker with a golang plugin that resides at \n~/plugin/plugin.so\n on the host you can use a bind mount\n\n\n\ndocker run --rm --net=host -v ~/plugin:/tmp/plugin rwynn/monstache:4.6.4.cgo -mapper-plugin-path /tmp/plugin/plugin.so\n\n\n\n\n\nShells \n Binaries\n\n\nImages ending with \n.cgo\n like: \nrwynn/monstache:4.4.0.cgo\n have both: \nsh\n \n \nbash\n shells, but don't have \nwget\n or \ncurl\n binaries\n\n\nImages with just the version number like: \nrwynn/monstache:4.4.0\n have only \nsh\n shell and \nwget\n binary\n\n\nThis information might be useful if you \nexec\n in the container or use \nhealthcheck\n\n\nHTTP Server\n\n\nMonstache has a built in HTTP server that you can enable with --enable-http-server. It\nlistens on :8080 by default but you can change this with --http-server-addr.\n\n\nWhen using monstache with kubernetes this server can be used to detect liveness and \n\nact accordingly\n\n\nThe following GET endpoints are available\n\n\n/started\n\n\nReturns the uptime of the server\n\n\n/healthz\n\n\nReturns at 200 status code with the text \"ok\" when monstache is running\n\n\n/stats\n\n\nReturns the current indexing statistics in JSON format. Only available if stats are enabled\n\n\n/config\n\n\nReturns the configuration monstache is using in JSON format", 
            "title": "Advanced"
        }, 
        {
            "location": "/advanced/#advanced", 
            "text": "", 
            "title": "Advanced"
        }, 
        {
            "location": "/advanced/#gridfs-support", 
            "text": "Monstache supports indexing the raw content of files stored in GridFS into Elasticsearch for full\ntext search.  This feature requires that you install an Elasticsearch plugin which enables the field type  attachment .\nFor versions of Elasticsearch prior to version 5 you should install the  mapper-attachments  plugin.\nFor version 5 or later of Elasticsearch you should instead install the  ingest-attachment  plugin.  Once you have installed the appropriate plugin for Elasticsearch, getting file content from GridFS into Elasticsearch is\nas simple as configuring monstache.  You will want to enable the  index-files  option and also tell monstache the \nnamespace of all collections which will hold GridFS files. For example in your TOML config file,  index-files = true\n\ndirect-read-namespaces = [ users.fs.files ,  posts.fs.files ]\n\nfile-namespaces = [ users.fs.files ,  posts.fs.files ]\n\nfile-highlighting = true  The above configuration tells monstache that you wish to index the raw content of GridFS files in the  users  and  posts \nMongoDB databases. By default, MongoDB uses a bucket named  fs , so if you just use the defaults your collection name will\nbe  fs.files .  However, if you have customized the bucket name, then your file collection would be something like  mybucket.files \nand the entire namespace would be  users.mybucket.files .  When you configure monstache this way it will perform an additional operation at startup to ensure the destination indexes in\nElasticsearch have a field named  file  with a type mapping of  attachment .    For the example TOML configuration above, monstache would initialize 2 indices in preparation for indexing into\nElasticsearch by issuing the following REST commands:  For Elasticsearch versions prior to version 5...  POST /users.fs.files\n{\n  \"mappings\": {\n    \"fs.files\": {\n      \"properties\": {\n    \"file\": { \"type\": \"attachment\" }\n}}}}\n\nPOST /posts.fs.files\n{\n  \"mappings\": {\n    \"fs.files\": {\n      \"properties\": {\n    \"file\": { \"type\": \"attachment\" }\n}}}}  For Elasticsearch version 5 and above...  PUT /_ingest/pipeline/attachment\n{\n  \"description\" : \"Extract file information\",\n  \"processors\" : [\n    {\n      \"attachment\" : {\n    \"field\" : \"file\"\n      }\n    }\n  ]\n}  When a file is inserted into MongoDB via GridFS, monstache will detect the new file, use the MongoDB api to retrieve the raw\ncontent, and index a document into Elasticsearch with the raw content stored in a  file  field as a base64 \nencoded string. The Elasticsearch plugin will then extract text content from the raw content using  Apache Tika , tokenize the text content, and allow you to query on the content of the file.  To test this feature of monstache you can simply use the  mongofiles \ncommand to quickly add a file to MongoDB via GridFS.  Continuing the example above one could issue the following command to put a \nfile named  resume.docx  into GridFS and after a short time this file should be searchable in Elasticsearch in the index  users.fs.files .  mongofiles -d users put resume.docx  After a short time you should be able to query the contents of resume.docx in the users index in Elasticsearch  curl -XGET \"http://localhost:9200/users.fs.files/_search?q=golang\"  If you would like to see the text extracted by Apache Tika you can project the appropriate sub-field  For Elasticsearch versions prior to version 5...  curl -H \"Content-Type:application/json\" localhost:9200/users.fs.files/_search?pretty -d '{\n    \"fields\": [ \"file.content\" ],\n    \"query\": {\n        \"match\": {\n            \"file.content\": \"golang\"\n        }\n    }\n}'  For Elasticsearch version 5 and above...  curl -H \"Content-Type:application/json\" localhost:9200/users.fs.files/_search?pretty -d '{\n    \"_source\": [ \"attachment.content\" ],\n    \"query\": {\n        \"match\": {\n            \"attachment.content\": \"golang\"\n        }\n    }\n}'  When  file-highlighting  is enabled you can add a highlight clause to your query  For Elasticsearch versions prior to version 5...  curl -H \"Content-Type:application/json\" localhost:9200/users.fs.files/_search?pretty -d '{\n    \"fields\": [\"file.content\"],\n    \"query\": {\n        \"match\": {\n            \"file.content\": \"golang\"\n        }\n    },\n    \"highlight\": {\n        \"fields\": {\n            \"file.content\": {\n            }\n        }\n    }\n}'  For Elasticsearch version 5 and above...  curl -H \"Content-Type:application/json\" localhost:9200/users.fs.files/_search?pretty -d '{\n    \"_source\": [\"attachment.content\"],\n    \"query\": {\n        \"match\": {\n            \"attachment.content\": \"golang\"\n        }\n    },\n    \"highlight\": {\n        \"fields\": {\n            \"attachment.content\": {\n            }\n        }\n    }\n}'  The highlight response will contain emphasis on the matching terms  For Elasticsearch versions prior to version 5...  \"hits\" : [ {\n    \"highlight\" : {\n        \"file.content\" : [ \"I like to program in  em golang /em .\\n\\n\" ]\n    }\n} ]  For Elasticsearch version 5 and above...  \"hits\" : [{\n    \"highlight\" : {\n        \"attachment.content\" : [ \"I like to program in  em golang /em .\" ]\n    }\n}]", 
            "title": "GridFS Support"
        }, 
        {
            "location": "/advanced/#workers", 
            "text": "You can run multiple monstache processes and distribute the work between them.  First configure\nthe names of all the workers in a shared config.toml file.  workers = [ Tom ,  Dick ,  Harry ]  In this case we have 3 workers.  Now we can start 3 monstache processes and give each one of the worker\nnames.  monstache -f config.toml -worker Tom\nmonstache -f config.toml -worker Dick\nmonstache -f config.toml -worker Harry  monstache will hash the id of each document using consistent hashing so that each id is handled by only\none of the available workers.", 
            "title": "Workers"
        }, 
        {
            "location": "/advanced/#high-availability", 
            "text": "You can run monstache in high availability mode by starting multiple processes with the same value for  cluster-name .\nEach process will join a cluster which works together to ensure that a monstache process is always syncing to Elasticsearch.  High availability works by ensuring one active process in the  monstache.cluster  collection in mongodb at any given time. Only the process in\nthis collection will be syncing for the cluster.  Processes not present in this collection will be paused.  Documents in the  monstache.cluster  collection have a TTL assigned to them.  When a document in this collection times out it will be removed from\nthe collection by mongodb and another process in the monstache cluster will have a chance to write to the collection and become the\nnew active process.  When  cluster-name  is supplied the  resume  feature is automatically turned on and the  resume-name  becomes the name of the cluster.\nThis is to ensure that each of the processes is able to pick up syncing where the last one left off.    You can combine the HA feature with the workers feature.  For 3 cluster nodes with 3 workers per node you would have something like the following:  // config.toml\nworkers = [\"Tom\", \"Dick\", \"Harry\"]\n\n// on host A\nmonstache -cluster-name HA -worker Tom -f config.toml\nmonstache -cluster-name HA -worker Dick -f config.toml\nmonstache -cluster-name HA -worker Harry -f config.toml\n\n// on host B\nmonstache -cluster-name HA -worker Tom -f config.toml\nmonstache -cluster-name HA -worker Dick -f config.toml\nmonstache -cluster-name HA -worker Harry -f config.toml\n\n// on host C\nmonstache -cluster-name HA -worker Tom -f config.toml\nmonstache -cluster-name HA -worker Dick -f config.toml\nmonstache -cluster-name HA -worker Harry -f config.toml  When the clustering feature is combined with workers then the  resume-name  becomes the cluster name concatenated with the worker name.", 
            "title": "High Availability"
        }, 
        {
            "location": "/advanced/#index-mapping", 
            "text": "When indexing documents from MongoDB into elasticsearch the default mapping is as follows:  For Elasticsearch prior to 6.2  elasticsearch index name     = mongodb database name . mongodb collection name\nelasticsearch type           = mongodb collection name\nelasticsearch document _id   = mongodb document _id  For Elasticsearch 6.2+  elasticsearch index name     = mongodb database name . mongodb collection name\nelasticsearch type           = _doc \nelasticsearch document _id   = mongodb document _id  If these default won't work for some reason you can override the index and type mapping on a per collection basis by adding\nthe following to your TOML config file:  [[mapping]]\nnamespace =  test.test \nindex =  index1 \ntype =  type1 \n\n[[mapping]]\nnamespace =  test.test2 \nindex =  index2 \ntype =  type2   With the configuration above documents in the  test.test  namespace in MongoDB are indexed into the  index1  \nindex in elasticsearch with the  type1  type.  If you need your index and type mapping to be more dynamic, such as based on values inside the MongoDB document, then\nsee the sections  Middleware  and   Routing .   Warning  It is not recommended to override the default type of  _doc  if using Elasticsearch 6.2+ since this will be the supported path going forward.\nAlso, using  _doc  as the type will not work with Elasticsearch prior to 6.2.   Make sure that automatic index creation is not disabled in elasticsearch.yml or create your target indexes before using Monstache.  If automatic index creation must be controlled, whitelist any indexes in elasticsearch.yml that monstache will create.", 
            "title": "Index Mapping"
        }, 
        {
            "location": "/advanced/#namespaces", 
            "text": "When a document is inserted, updated, or deleted in mongodb a document is appended to the oplog representing the event.  This document has a field  ns  which is the namespace.  For inserts, updates, and deletes the namespace is the database name and collection name of the document changed joined by a dot. E.g. for  use test; db.foo.insert({hello: \"world\"});  the namespace for the event in the oplog would be  test.foo .  In addition to inserts, updates, and deletes monstache also supports database and collection drops.  When a database or collection is dropped in mongodb an event is appended to the oplog.  Like the other types of changes this event has a field  ns  representing the namespace.  However, for drops the namespace is the database name and the string  $cmd  joined by a dot.  E.g. for  use test; db.foo.drop()  the namespace for the event in the oplog would be  test.$cmd .    When configuring namespaces in monstache you will need to account for both cases.     Warning  Be careful if you have configured  dropped-databases=true  or  dropped-collections=true  AND you also have a  namespace-regex  set.  If your namespace regex does not take into account the  db.$cmd  namespace the event may be filtered and the elasticsearch index not deleted on a drop.", 
            "title": "Namespaces"
        }, 
        {
            "location": "/advanced/#middleware", 
            "text": "monstache supports embedding user defined middleware between MongoDB and Elasticsearch.  middleware is able to transform documents,\ndrop documents, or define indexing metadata.  middleware may be written in either Javascript or in Golang as a plugin.  Golang plugins\nrequire Go version 1.8 or greater on Linux. currently, you are able to use Javascript or Golang but not both (this may change in the future).", 
            "title": "Middleware"
        }, 
        {
            "location": "/advanced/#golang", 
            "text": "monstache supports Golang 1.8+ plugins on Linux.  To implement a plugin for monstache you simply need to implement a specific function signature,\nuse the go command to build a .so file for your plugin, and finally pass the path to your plugin .so file when running monstache.  To create a golang plugin for monstache   get the necessary dependencies with  go get -u github.com/rwynn/monstache/monstachemap  create a .go source file that belongs to the package  main  import  github.com/rwynn/monstache/monstachemap  implement a function named  Map  with the following signature   func Map(input *monstachemap.MapperPluginInput) (output *monstachemap.MapperPluginOutput, err error)   optionally implement a function named  Filter  with the following signature   func Filter(input *monstachemap.MapperPluginInput) (keep bool, err error)  plugins can be compiled using  go build -buildmode=plugin -o myplugin.so myplugin.go  to enable the plugin, start monstache with  monstache -mapper-plugin-path /path/to/myplugin.so  the following example plugin simply converts top level string values to uppercase  package main\nimport (\n     github.com/rwynn/monstache/monstachemap \n     strings \n)\n// a plugin to convert document values to uppercase\nfunc Map(input *monstachemap.MapperPluginInput) (output *monstachemap.MapperPluginOutput, err error) {\n    doc := input.Document\n    for k, v := range doc {\n        switch v.(type) {\n        case string:\n            doc[k] = strings.ToUpper(v.(string))\n        }\n    }\n    output =  monstachemap.MapperPluginOutput{Document: doc}\n    return\n}  The input parameter will contain information about the document's origin database and collection.  To drop the document (direct monstache not to index it) set  output.Drop = true .  To simply pass the original document through to Elasticsearch, set  output.Passthrough = true  To set custom indexing metadata on the document use  output.Index ,  output.Type ,  output.Parent  and  output.Routing .   Note  If you override  output.Parent  or  output.Routing  for any MongoDB namespaces in a \ngolang plugin you should also add those namespaces to the  routing-namespaces  array \nin your config file.  This instructs Monstache to save the routing information so that deletes of the document work\ncorrectly.   If would like to embed other MongoDB documents (possibly from a different collection) within the current document \nbefore indexing, you can access the  *mgo.Session  pointer as  input.Session .  With the mgo session you can use the  mgo API  to find documents in MongoDB and embed them in the Document set on output.  When you implement a  Filter  function the function is called immediately after reading inserts and updates from the oplog.  You can return false from this function to completely ignore a document.  This is different than setting  output.Drop  from the mapping function because when you set  output.Drop  to true, a delete request is issued to Elasticsearch in case the document had previously been indexed.  By contrast, returning false from the  Filter  function causes the operation to be completely ignored and there is no corresponding delete request issued to Elasticsearch.", 
            "title": "Golang"
        }, 
        {
            "location": "/advanced/#javascript", 
            "text": "", 
            "title": "Javascript"
        }, 
        {
            "location": "/advanced/#transformation", 
            "text": "Monstache uses the amazing  otto  library to provide transformation at the document field\nlevel in Javascript.  You can associate one javascript mapping function per mongodb collection.  You can also associate a function \nat the global level by not specifying a namespace.  These javascript functions are\nadded to your TOML config file, for example:  [[script]]\nnamespace =  mydb.mycollection \nscript =  \nvar counter = 1;\nmodule.exports = function(doc) {\n    doc.foo +=  test  + counter;\n    counter++;\n    return doc;\n} \n\n[[script]]\nnamespace =  anotherdb.anothercollection \npath =  path/to/transform.js \nrouting = true\n\n[[script]]\n# this script does not declare a namespace\n# it is global to all collections\nscript =  \nmodule.exports = function(doc, ns) {\n    // the doc namespace e.g. test.test is passed as the 2nd arg\n    return _.omit(doc,  password ,  secret );\n}   The example TOML above configures 3 scripts. The first is applied to  mycollection  in  mydb  while the second is applied\nto  anothercollection  in  anotherdb . The first script is inlined while the second is loaded from a file path.  The path can be absolute or relative to the directory monstache is executed from.\nThe last script does not specify a namespace, so documents from all collections pass through it. Global scripts are run before scripts which are\nlinked to a specific namespace.  You will notice that the multi-line string feature of TOML is used to assign a javascript snippet to the variable named script .  The javascript assigned to script must assign a function to the exports property of the  module  object.  This \nfunction will be passed the document from mongodb just before it is indexed in elasticsearch.  Inside the function you can\nmanipulate the document to drop fields, add fields, or augment the existing fields.  The  this  reference in the mapping function is assigned to the document from mongodb.    When the return value from the mapping function is an  object  then that mapped object is what actually gets indexed in elasticsearch.\nFor these purposes an object is a javascript non-primitive, excluding  Function ,  Array ,  String ,  Number ,  Boolean ,  Date ,  Error  and  RegExp .", 
            "title": "Transformation"
        }, 
        {
            "location": "/advanced/#filtering", 
            "text": "You can completely ignore documents by adding filter configurations to your TOML config file.  The filter functions are executing immediately after inserts or updates are read from the oplog.  The correspding document is passed into the function and you can return true or false to include or ignore the document.  [[filter]]\nnamespace =  db.collection \nscript =  \nmodule.exports = function(doc) {\n    return !!doc.interesting;\n} \n\n[[filter]]\nnamespace =  db2.collection2 \npath =  path/to/script.js", 
            "title": "Filtering"
        }, 
        {
            "location": "/advanced/#dropping", 
            "text": "If the return value from the mapping function is not an  object  per the definition above then the result is converted into a  boolean \nand if the boolean value is  false  then that indicates to monstache that you would not like to index the document. If the boolean value is  true  then\nthe original document from mongodb gets indexed in elasticsearch.  This allows you to return false or null if you have implemented soft deletes in mongodb.  [[script]]\nnamespace =  db.collection \nscript =  \nmodule.exports = function(doc) {\n    if (!!doc.deletedAt) {\n        return false;\n    }\n    return true;\n}   In the above example monstache will index any document except the ones with a  deletedAt  property.  If the document is first\ninserted without a  deletedAt  property, but later updated to include the  deletedAt  property then monstache will remove, or drop, the\npreviously indexed document from the elasticsearch index.    Note  Dropping a document is different that filtering a document.  A filtered document is completely ignored.  A dropped document results in a delete request being issued to Elasticsearch in case the document had previously been indexed.", 
            "title": "Dropping"
        }, 
        {
            "location": "/advanced/#scripting-features", 
            "text": "You may have noticed that in the first example above the exported mapping function closes over a var named  counter .  You can\nuse closures to maintain state between invocations of your mapping function.  Finally, since Otto makes it so easy, the venerable  Underscore  library is included for you at \nno extra charge.  Feel free to abuse the power of the  _ .", 
            "title": "Scripting Features"
        }, 
        {
            "location": "/advanced/#embedding-documents", 
            "text": "In your javascript function you have access to the following global functions to retreive documents from MongoDB for\nembedding in the current document before indexing.  Using this approach you can pull in related data.  function findId(documentId, [options]) {\n    // convenience method for findOne({_id: documentId})\n    // returns 1 document or null\n}\n\nfunction findOne(query, [options]) {\n    // returns 1 document or null\n}\n\nfunction find(query, [options]) {\n    // returns an array of documents or null\n}  Each function takes a  query  object parameter and an optional  options  object parameter.  The options object takes the following keys and values:  var options = {\n    database:  test ,\n    collection:  test ,\n    // to omit _id set the _id key to 0 in select\n    select: {\n        age: 1\n    },\n    // only applicable to find...\n    sort: [ name ],\n    limit: 2\n}  If the database or collection keys are omitted from the options object, the values for database and/or\ncollection are set to the database and collection of the document being processed.  Here are some examples:  This example sorts the documents in the same collection as the document being processed by name and returns\nthe first 2 documents projecting only the age field.  The result is set on the current document before being\nindexed.  [[script]]\nnamespace =  test.test \nscript =  \nmodule.exports = function(doc) {\n    doc.twoAgesSortedByName = find({}, {\n            sort: [ name ],\n            limit: 2,\n            select: {\n              age: 1\n            }\n    });\n    return doc;\n}   This example grabs a reference id from a document and replaces it with the corresponding document with that id.  [[script]]\nnamespace =  test.posts \nscript =  \nmodule.exports = function(post) {\n    if (post.author) { // author is a an object id reference\n        post.author = findId(post.author, {\n          database:  test ,\n          collection:  users \n        });\n    }\n    return post;\n}", 
            "title": "Embedding Documents"
        }, 
        {
            "location": "/advanced/#indexing-metadata", 
            "text": "You can override the indexing metadata for an individual document by setting a special field named _meta_monstache  on the document you return from your Javascript function.  Assume there is a collection in MongoDB named  company  in the  test  database.\nThe documents in this collection look like either  {  _id :  london ,  type :  branch ,  name :  London Westminster ,  city :  London ,  country :  UK  }  or  {  _id :  alice ,  type :  employee ,  name :  Alice Smith ,  branch :  london  }  Given the above the following snippet sets up a parent-child relationship in Elasticsearch based on the\nincoming documents from MongoDB and updates the ns (namespace) from test.company to company in elasticsearch  [[script]]\nnamespace =  test.company \nrouting = true\nscript =  \nmodule.exports = function(doc, ns) {\n        // var meta = { type: doc.type, index: 'company' };\n    var meta = { type: doc.type, index: ns.split( . )[1] };\n    if (doc.type ===  employee ) {\n        meta.parent = doc.branch;\n    }\n    doc._meta_monstache = meta;\n    return _.omit(doc,  branch ,  type );\n}   The snippet above will route these documents to the  company  index in Elasticsearch instead of the\ndefault of  test.company , if you didn't specify a namespace, it'll route all documents to indexes named as the collection only without the database  db . collection  (mongodb) =   collection  (elasticsearch).  Also, instead of using  company  as the Elasticsearch type, the type\nattribute from the document will be used as the Elasticsearch type.  Finally, if the type is\nemployee then the document will be indexed as a child of the branch the person belongs to.  We can throw away the type and branch information by deleting it from the document before returning\nsince the type information will be stored in Elasticsearch under  _type  and the branch information\nwill be stored under  _parent .  The example is based on the Elasticsearch docs for  parent-child  For more on updating the namespace name, check the  Delete Strategy  as there is a change of the default behviour in v4.4.0   v3.11.0", 
            "title": "Indexing Metadata"
        }, 
        {
            "location": "/advanced/#routing", 
            "text": "Routing is the process by which Elasticsearch determines which shard a document will reside in.  Monstache\nsupports user defined, or custom, routing of your MongoDB documents into Elasticsearch.    Consider an example where you have a  comments  collection in MongoDB which stores a comment and \nits associated post identifier.    use blog;\ndb.comments.insert({title:  Did you read this? , post_id:  123 });\ndb.comments.insert({title:  Yeah, it's good , post_id:  123 });  In this case monstache will index those 2 documents in an index named  blog.comments  under the id\ncreated by MongoDB.  When Elasticsearch routes a document to a shard, by default, it does so by hashing \nthe id of the document.  This means that as the number of comments on post  123  grows, each of the comments\nwill be distributed somewhat evenly between the available shards in the cluster.    Thus, when a query is performed searching among the comments for post  123  Elasticsearch will need to query\nall of those shards just in case a comment happened to have been routed there.  We can take advantage of the support in Elasticsearch and in monstache to do some intelligent\nrouting such that all comments for post  123  reside in the same shard.  First we need to tell monstache that we would like to do custom routing for this collection by setting  routing \nequal to true on a custom script for the namespace.  Then we need to add some metadata to the document telling\nmonstache how to route the document when indexing.  In this case we want to route by the  post_id  field.  [[script]]\nnamespace =  blog.comments \nrouting = true\nscript =  \nmodule.exports = function(doc) {\n    doc._meta_monstache = { routing: doc.post_id };\n    return doc;\n}   Now when monstache indexes document for the collection  blog.comments  it will set the special  _routing  attribute\nfor the document on the index request such that Elasticsearch routes comments based on their corresponding post.   The  _meta_monstache  field is used only to inform monstache about routing and is not included in the source\ndocument when indexing to Elasticsearch.    Now when we are searching for comments and we know the post id that the comment belongs to we can include that post\nid in the request and make a search that normally queries all shards query only 1 shard.  $ curl -H  Content-Type:application/json  -XGET 'http://localhost:9200/blog.comments/_search?routing=123' -d '\n{\n    query :{\n       match_all :{}\n   }\n}'  You will notice in the response that only 1 shard was queried instead of all your shards.  Custom routing is great\nway to reduce broadcast searches and thus get better performance.  The catch with custom routing is that you need to include the routing parameter on all insert, update, and delete\noperations.  Insert and update is not a problem for monstache because the routing information will come from your\nMongoDB document.  Deletes, however, pose a problem for monstache because when a delete occurs in MongoDB the\ninformation in the oplog is limited to the id of the document that was deleted.  But monstache needs to know where the\ndocument was originally routed in order to tell Elasticsearch where to look for it.  Monstache has 3 available strategies for handling deletes in this situation.  The default strategy is stateless and uses\na term query into Elasticsearch based on the ID of the document deleted in MongoDB.  If the search into Elasticsearch returns\nexactly 1 document then monstache will schedule that document for deletion. The 2nd stategy monstache uses is stateful and requires\ngiving monstache the ability to write to the collection  monstache.meta .  In this collection monstache stores information about\ndocuments that were given custom indexing metadata.  This stategy slows down indexing and takes up space in MongoDB. \nHowever, it is precise because it records exactly how each document was indexed. The final stategy simply punts on deletes and\nleaves document deletion to the user.  If you don't generally delete documents in MongoDB or don't care if Elasticsearch contains\ndocuments which have been deleted in MongoDB, this option is available. See  Delete Strategy  for more information.  For more information see  Customizing Document Routing  In addition to letting your customize the shard routing for a specific document, you can also customize the elasticsearch index  and  type  using a script by putting the custom information in the meta attribute.   [[script]]\nnamespace =  blog.comments \nrouting = true\nscript =  \nmodule.exports = function(doc) {\n    if (doc.score  = 100) {\n        // NOTE: prefix dynamic index with namespace for proper cleanup on drops\n        doc._meta_monstache = { index:  blog.comments.highscore , type:  highScoreComment , routing: doc.post_id };\n    } else {\n        doc._meta_monstache = { routing: doc.post_id };\n    }\n    return doc;\n}", 
            "title": "Routing"
        }, 
        {
            "location": "/advanced/#joins", 
            "text": "Elasticsearch 6 introduces an updated approach to parent-child called joins.  The following example shows how you can accomplish joins\nwith Monstache.  The example is based on the Elasticsearch  documentation .  This example assumes Monstache is syncing the  test.test  collection in MongoDB with the  test.test  index in Elasticsearch.  First we will want to setup an index mapping in Elasticsearch describing the join field.    curl -XPUT 'localhost:9200/test.test?pretty' -H 'Content-Type: application/json' -d'\n{\n   mappings : {\n     _doc : {\n       properties : {\n         my_join_field : { \n           type :  join ,\n           relations : {\n             question :  answer  \n          }\n        }\n      }\n    }\n  }\n}\n'   Warning  The above mapping uses _doc as the Elasticsearch type. _doc is the recommended type for new\nversions Elasticsearch but it only works with Elasticsearch versions 6.2 and greater.  Monstache\ndefaults to using _doc as the type when it detects Elasticsearch version 6.2 or greater.  If you\nare using a previous version of Elasticsearch monstache defaults to using the MongoDB collection\nname as the Elasticsearch type. The type Monstache uses can be overriden but it is not\nrecommended from Elasticsearch 6.2 on.    Next will will configure Monstache with custom Javascript middleware that does transformation and routing.  In a file called CONFIG.toml.  [[script]]\nnamespace =  test.test \nrouting = true\nscript =  \nmodule.exports = function(doc) {\n        var routing;\n        if (doc.type ===  question ) {\n                routing = doc._id;\n                doc.my_join_field = {\n                   name:  question \n                }\n        } else if (doc.type ===  answer ) {\n                routing = doc.question;\n                doc.my_join_field = {\n                  name:  answer ,\n                  parent: routing\n                };\n        }\n        if (routing) {\n                doc._meta_monstache = { routing: routing };\n        }\n        return doc;\n}   The mapping function adds a  my_join_field  field to each document.  The contents of the field are based on the  type  attribute in the MongoDB\ndocument. Also, the function ensures that the routing is always based on the _id of the question document.     Now with this config in place we can start Monstache.  We will use verbose to see the requests.  monstache -verbose -f CONFIG.toml  With Monstache running we are now ready to insert into MongoDB  \nrs:PRIMARY  use test;\nswitched to db test\n\nrs:PRIMARY  db.test.insert({type:  question , text:  This is a question });\n\nrs:PRIMARY  db.test.find()\n{  _id  : ObjectId( 5a84a8b826993bde57c12893 ),  type  :  question ,  text  :  This is a question  }\n\nrs:PRIMARY  db.test.insert({type:  answer , text:  This is an answer , question: ObjectId( 5a84a8b826993bde57c12893 ) });  When we insert these documents we should see Monstache generate the following requests to Elasticsearch  \n{ index :{ _id : 5a84a8b826993bde57c12893 , _index : test.test , _type : _doc , routing : 5a84a8b826993bde57c12893 , version :6522523668566769665, version_type : external }}\n{ my_join_field :{ name : question }, text : This is a question , type : question }\n\n{ index :{ _id : 5a84a92b26993bde57c12894 , _index : test.test , _type : _doc , routing : 5a84a8b826993bde57c12893 , version :6522524162488008705, version_type : external }}\n{ my_join_field :{ name : answer , parent : 5a84a8b826993bde57c12893 }, question : 5a84a8b826993bde57c12893 , text : This is an answer , type : answer }  This looks good.  We should now have a parent/child relationship between these documents in Elasticsearch.  If we do a search on the  test.test  index we see the following results:  \n  hits  : {\n     total  : 2,\n     max_score  : 1.0,\n     hits  : [\n      {\n         _index  :  test.test ,\n         _type  :  _doc ,\n         _id  :  5a84a8b826993bde57c12893 ,\n         _score  : 1.0,\n         _routing  :  5a84a8b826993bde57c12893 ,\n         _source  : {\n           my_join_field  : {\n             name  :  question \n          },\n           text  :  This is a question ,\n           type  :  question \n        }\n      },\n      {\n         _index  :  test.test ,\n         _type  :  _doc ,\n         _id  :  5a84a92b26993bde57c12894 ,\n         _score  : 1.0,\n         _routing  :  5a84a8b826993bde57c12893 ,\n         _source  : {\n           my_join_field  : {\n             name  :  answer ,\n             parent  :  5a84a8b826993bde57c12893 \n          },\n           question  :  5a84a8b826993bde57c12893 ,\n           text  :  This is an answer ,\n           type  :  answer \n        }\n      }\n    ]\n  }  To clean up our documents in Elasticsearch a bit we can omit the information that we don't really need in the source docs by \nupdating our mapping function. This information needs not be at the top-level since it is duplicated in  my_join_field .      return _.omit(doc,  type ,  question );  If your parent and child documents are in separate MongoDB collections then you would set up a script\nfor each collection.  You can tell if the doc is a parent or child by the collection it comes from. The only other difference would be that you would need to override the index dynamically in addition to the routing such that documents from both MongoDB collections target the same \nindex.      doc._meta_monstache = { routing: routing, index:  parentsAndChildren  };   Warning  You must be careful when you route 2 or more MongoDB collections to the same Elasticsearch index\nthat the document _ids across the MongoDB collections do not collide for any 2 docs because \nthey will be used as the _id in the target index.", 
            "title": "Joins"
        }, 
        {
            "location": "/advanced/#time-machines", 
            "text": "If you are not just interested in what the current value of a document in MongoDB is, but also would like to see how it has changed over time use  time machine namespaces .   For example, you've inserted and later updated a document with id 123 in the  test.test  collection in MongoDB. If  test.test  is a time machine namespace you will have 2 documents representing those changes in the  log.test.test.2018-02-20  index (timestamp will change) in Elasticsearch.  If you later want all the changes made to that document in MongoDB you can issue a query like this:  $ curl -XGET 'http://localhost:9200/log.test.test.*/_search?routing=123' -d '\n{\n    query :{\n       sort  : [\n        {  _oplog_ts  : { order  :  desc }}\n      ],\n       filtered :{\n          query :{\n             match_all :{}\n         },\n          filter :{\n             term :{\n                _source_id : 123 \n            }\n         }\n      }\n   }\n}'  That query will be very efficient because it only queries the shard that all the change docs went to for MongoDB document id 123.  It filters the documents on that shard by  _source_id , or id from MongoDB, to only give us the changes to that document.  Finally, it sorts by the  _oplog_ts  which gives us the most recent change docs first.     The index pattern in the query is a wildcard to pick up all the timestamped indexes that we've acculated for the  test.test  namespace.", 
            "title": "Time Machines"
        }, 
        {
            "location": "/advanced/#merge-patches", 
            "text": "A unique feature of monstache is support for JSON Merge Patches  rfc-7396 .  If merge patches are enabled monstache will add an additional field to documents indexed into elasticsearch. The\nname of this field is configurable but it defaults to  json-merge-patches .    Consider the following example with merge patches enabled...   db.test.insert({name:  Joe , age: 16, friends: [1, 2, 3]})  At this point you would have the following document source in elasticsearch.   \"_source\" : {\n  \"age\" : 16,\n  \"friends\" : [\n    1,\n    2,\n    3\n  ],\n  \"json-merge-patches\" : [\n    {\n      \"p\" : \"{\\\"age\\\":16,\\\"friends\\\":[1,2,3],\\\"name\\\":\\\"Joe\\\"}\",\n      \"ts\" : 1487263414,\n      \"v\" : 1\n    }\n  ],\n  \"name\" : \"Joe\"\n}  As you can see we have a single timestamped merge patch in the json-merge-patches array.    Now let's update the document to remove a friend and update the age.  db.test.update({name:  Joe }, {$set: {age: 21, friends: [1, 3]}})  If we now look at the document in elasticsearch we see the following:  \"_source\" : {\n  \"age\" : 21,\n  \"friends\" : [\n    1,\n    3\n  ],\n  \"json-merge-patches\" : [\n    {\n      \"p\" : \"{\\\"age\\\":16,\\\"friends\\\":[1,2,3],\\\"name\\\":\\\"Joe\\\"}\",\n      \"ts\" : 1487263414,\n      \"v\" : 1\n    },\n    {\n      \"p\" : \"{\\\"age\\\":21,\\\"friends\\\":[1,3]}\",\n      \"ts\" : 1487263746,\n      \"v\" : 2\n    }\n  ],\n  \"name\" : \"Joe\"\n}  You can see that the document was updated as expected and an additional merge patch was added.  Each time the document is updated in mongodb the corresponding document in elasticsearch gains a\ntimestamped merge patch.  Using this information we can time travel is the document's history.  There is a merge patch for each version of the document.  To recreate a specific version we simply need\nto apply the merge patches in order up to the version that we want.  To get version 1 of the document above we start with {} and apply the 1st merge patch.    To get version 2 of the document above we start with {}   apply the 1st merge patch to get v1  apply the 2nd merge patch to v1 to get v2   The timestamps associated with these merge patches are in seconds since the epoch, taken from the\ntimestamp recorded in the oplog when the insert or update occured.   To enable the merge patches feature in monstache you need to add the following to you TOML config:  enable-patches = true\npatch-namespaces = [\"test.test\"]  You need you add each namespace that you would like to see patches for in the patch-namespaces array.  Optionally, you can change the key under which the patches are stored in the source document as follows:  merge-patch-attribute = \"custom-merge-attr\"  Merge patches will only be recorded for data read from the MongoDB oplog.  Data read using the direct read\nfeature will not be enhanced with merge patches.  Most likely, you will want to turn off indexing for the merge patch attribute.  You can do this by creating\nan index template for each patch namespace before running monstache...  PUT /_template/test.test\n{\n    \"template\" : \"test.test\",\n    \"mappings\" : {\n    \"test\" : {\n        \"json-merge-patches\" : { \"index\" : false }\n    }\n    }\n}", 
            "title": "Merge Patches"
        }, 
        {
            "location": "/advanced/#systemd", 
            "text": "Monstache has support built in for integrating with systemd. The following  monstache.service  is an example \nsystemd configuration.  [Unit]\nDescription=monstache sync service\n\n[Service]\nType=notify\nExecStart=/usr/local/bin/monstache\nWatchdogSec=30s\nRestart=on-failure\n\n[Install]\nWantedBy=multi-user.target  System-d unit files are normally saved to  /lib/systemd/system .    After saving the monstache.service file you can run  systemctl daemon-reload  to tell systemd to reload \nall unit files.   You can enable the service to start on boot with  systemctl enable monstache.service  and start the service with  systemctl start monstache.service .  With the configuration above monstache will notify systemd when it has started successfully and then notify \nsystemd repeatedly at half the WatchDog interval to signal liveness.  The configuration above causes systemd\nto restart monstache if it does not start or respond within the WatchdDog interval.", 
            "title": "Systemd"
        }, 
        {
            "location": "/advanced/#docker", 
            "text": "There are Docker images available for Monstache on  Docker Hub  You can pull and run the latest images with  # for elasticsearch   5\ndocker run rwynn/monstache:latest -v\n\n# for elasticsearch  = 5\ndocker run rwynn/monstache:rel3 -v  You can pull and run release images with  docker run rwynn/monstache:4.6.4 -v\n\ndocker run rwynn/monstache:3.13.4 -v  For example, to run monstache via Docker with a golang plugin that resides at  ~/plugin/plugin.so  on the host you can use a bind mount  \ndocker run --rm --net=host -v ~/plugin:/tmp/plugin rwynn/monstache:4.6.4.cgo -mapper-plugin-path /tmp/plugin/plugin.so", 
            "title": "Docker"
        }, 
        {
            "location": "/advanced/#shells-binaries", 
            "text": "Images ending with  .cgo  like:  rwynn/monstache:4.4.0.cgo  have both:  sh     bash  shells, but don't have  wget  or  curl  binaries  Images with just the version number like:  rwynn/monstache:4.4.0  have only  sh  shell and  wget  binary  This information might be useful if you  exec  in the container or use  healthcheck", 
            "title": "Shells &amp; Binaries"
        }, 
        {
            "location": "/advanced/#http-server", 
            "text": "Monstache has a built in HTTP server that you can enable with --enable-http-server. It\nlistens on :8080 by default but you can change this with --http-server-addr.  When using monstache with kubernetes this server can be used to detect liveness and  act accordingly  The following GET endpoints are available", 
            "title": "HTTP Server"
        }, 
        {
            "location": "/advanced/#started", 
            "text": "Returns the uptime of the server", 
            "title": "/started"
        }, 
        {
            "location": "/advanced/#healthz", 
            "text": "Returns at 200 status code with the text \"ok\" when monstache is running", 
            "title": "/healthz"
        }, 
        {
            "location": "/advanced/#stats", 
            "text": "Returns the current indexing statistics in JSON format. Only available if stats are enabled", 
            "title": "/stats"
        }, 
        {
            "location": "/advanced/#config", 
            "text": "Returns the configuration monstache is using in JSON format", 
            "title": "/config"
        }, 
        {
            "location": "/about/", 
            "text": "About\n\n\n\n\nLicense\n\n\nThe MIT License (MIT)\n\n\nCopyright (c) 2016-2018 Ryan Wynn\n\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\nContributing\n\n\nThe Monstache project welcomes, and depends, on contributions from developers and\nusers in the open source community. Contributions can be made in a number of\nways, a few examples are:\n\n\n\n\nCode patches via pull requests\n\n\nDocumentation\n improvements\n\n\nBug reports\n and patch reviews\n\n\n\n\nReporting an Issue\n\n\nPlease include as much detail as you can. Let us know your platform, Monstache\nversion, MongoDB version, and Elasticsearch version.\n\n\nTesting the Development Version\n\n\nIf you want to just install and try out the latest development version of\nMonstache you can do so with the following command. This can be useful if you\nwant to provide feedback for a new feature or want to confirm if a bug you\nhave encountered is fixed in the git master.\n\n\ngo get -u github.com/rwynn/monstache\n\n\n\n\nRunning the tests\n\n\nTo run the tests, you will need to have local mongod and elasticsearch servers running.\n\nThen you will need to start a monstache process in one terminal or in the background.\n\n\nmonstache -verbose\n\n\n\n\nFinally in another terminal you can run the tests by issuing the following commands \n\n\ncd $GOPATH/src/github.com/rwynn/monstache\ngo test -v\n\n\n\n\n\n\nWarning\n\n\nRunning the Monstache tests will perform modifications to the \ntest.test\n namespace in \nMongoDB and will index documents in the \ntest.test\n index in Elasticsearch.  If you have\ndata that you need to keep on your local servers, make a back up before running the tests.\n\n\n\n\nSubmitting Pull Requests\n\n\nOnce you are happy with your changes or you are ready for some feedback, push\nit to your fork and send a pull request. For a change to be accepted it will\nmost likely need to have tests and documentation if it is a new feature.\n\n\nRelease Notes\n\n\nmonstache v4.6.3\n\n\n\n\nFix for issue #65, year outside of [0,9999].  Invalid time will be removed now with \nprune-invalid-json\n turned on\n\n\nFix for issue #62, the number of connections to MongoDB is now limited to a max of 32 per namespace\n\n\nFix for issue #59, unsupported values of +/- Infinity and NaN.  These values can now be removed with the \nprune-invalid-json\n setting\n\n\nFix for issue $46 and #66, having to do with filtering.  Filters now use locks to ensure the javascript environment is used by one at a time.\n\n\n\n\nmonstache v3.13.3\n\n\n\n\nFix for issue #65, year outside of [0,9999].  Invalid time will be removed now with \nprune-invalid-json\n turned on\n\n\nFix for issue #62, the number of connections to MongoDB is now limited to a max of 32 per namespace\n\n\nFix for issue #59, unsupported values of +/- Infinity and NaN.  These values can now be removed with the \nprune-invalid-json\n setting\n\n\nFix for issue $46 and #66, having to do with filtering.  Filters now use locks to ensure the javascript environment is used by one at a time.\n\n\n\n\nmonstache v4.6.2\n\n\n\n\nFix regression in 3.13 series where collections under 50K documents were not synching\n\n\nPerforming Tuning.  The following defaults have changed so please update your config files accordingly.\n\n\n\n\nelasticsearch-max-conns went from 10 -\n 4\nelasticsearch-max-docs went from 1000 -\n do not flush based on count (I suggest not overriding this since document sizes can vary greatly - instead use max-bytes)\nelasticsearch-max-bytes went from 5MB -\n 8MB\n\n\nNote when you specify elasticseach-max-bytes the value must be in bytes not MB\n\n\nmonstache v3.13.2\n\n\n\n\nFix regression in 3.13 series where collections under 50K documents were not synching\n\n\nPerforming Tuning.  The following defaults have changed so please update your config files accordingly.\n\n\n\n\nelasticsearch-max-conns went from 10 -\n 4\nelasticsearch-max-docs went from 1000 -\n do not flush based on count (I suggest not overriding this since document sizes can vary greatly - instead use max-bytes)\nelasticsearch-max-bytes went from 5MB -\n 8MB\n\n\nNote when you specify elasticseach-max-bytes the value must be in bytes not MB\n\n\nmonstache v4.6.1\n\n\n\n\nPerformance and bug fixes in the gtm library \n\n\n\n\nmonstache v3.13.1\n\n\n\n\nPerformance and bug fixes in the gtm library \n\n\n\n\nmonstache v4.6.0\n\n\n\n\nPerformance improvements. Much of the performance gains come from an upgrade of the gtm library. This library now uses split vector failing back to a paginated range queries. Also, some buffering has been removed at the gtm level for certain operations.\n\n\n\n\nmonstache v3.13.0\n\n\n\n\nPerformance improvements. Much of the performance gains come from an upgrade of the gtm library. This library now uses split vector failing back to a paginated range queries. Also, some buffering has been removed at the gtm level for certain operations.\n\n\n\n\nmonstache v4.5.0\n\n\n\n\nAdds an option \ndelete-index-pattern\n to specify an Elasticsearch index pattern to scope stateless deletes.  Indexes outside of this\n  pattern will not be considered when propogating deletes from MongoDB to Elasticsearch.  By default all Elasticsearch indexes are queried.\n\n\nAdds the ability to specify a global filter function in Javascript.  Previously, a filter function needed to be tied to a MongoDB namespace.\n  Now you can leave off the namespace and the filter function will be applied to all namespaces.  The filter function will receive the document\n  as the first argument and the MongoDB namespace as the second argument.\n\n\nBreaking change: \ndirect-read-cursors\n and \ndirect-read-batch-size\n have been removed as options.  The underlying gtm library of monstache has been\n  upgraded and no longer supports parallelCollectionScan which is being removed in future versions of MongoDB.  Now gtm will use splitVector to divy\n  up collections to read documents concurrently. Also, the batch size will be managed by MongoDB and not set explicitly.  See the gtm library docs for more information.\n\n\nAdds a boolean configuration option, \nprune-invalid-json\n, which defaults to false.  Set this to true if your MongoDB data has values such as +Inf,\n  -Inf, or NaN which are not supported by the golang JSON parser and cause infinite error loops to occur.  With prune-invalid-json set to true Monstache\n  will remove these values before indexing into Elasticsearch to avoid these errors.\n\n\n\n\nmonstache v3.12.0\n\n\n\n\nAdds an option \ndelete-index-pattern\n to specify an Elasticsearch index pattern to scope stateless deletes.  Indexes outside of this\n  pattern will not be considered when propogating deletes from MongoDB to Elasticsearch.  By default all Elasticsearch indexes are queried.\n\n\nAdds the ability to specify a global filter function in Javascript.  Previously, a filter function needed to be tied to a MongoDB namespace.\n  Now you can leave off the namespace and the filter function will be applied to all namespaces.  The filter function will receive the document\n  as the first argument and the MongoDB namespace as the second argument.\n\n\nBreaking change: \ndirect-read-cursors\n and \ndirect-read-batch-size\n have been removed as options.  The underlying gtm library of monstache has been\n  upgraded and no longer supports parallelCollectionScan which is being removed in future versions of MongoDB.  Now gtm will use splitVector to divy\n  up collections to read documents concurrently. Also, the batch size will be managed by MongoDB and not set explicitly.  See the gtm library docs for more information.\n\n\nAdds a boolean configuration option, \nprune-invalid-json\n, which defaults to false.  Set this to true if your MongoDB data has values such as +Inf,\n  -Inf, or NaN which are not supported by the golang JSON parser and cause infinite error loops to occur.  With prune-invalid-json set to true Monstache\n  will remove these values before indexing into Elasticsearch to avoid these errors.\n\n\n\n\nmonstache v4.4.0\n\n\n\n\n\n\nUpdated the default delete strategy\n\n\n\n\n\n\nBreaking change: check \ndelete-strategy\n\n\n\n\n\n\nmonstache v3.11.0\n\n\n\n\n\n\nUpdated the default delete strategy\n\n\n\n\n\n\nBreaking change: check \ndelete-strategy\n\n\n\n\n\n\nmonstache v4.3.2\n\n\n\n\nAllow specifying a script without a namespace. In this case documents from all collections will be run through the script. The document object will continue to be the 1st argument to the function and a new 2nd argument will be the namespace of the source document. Fixes #55.\n\n\nFix an issue where a Date object created in Javascript would not be formatted correctly for indexing.\n\n\nBuild with go 1.10.1\n\n\n\n\nmonstache v3.10.2\n\n\n\n\nAllow specifying a script without a namespace. In this case documents from all collections will be run through the script. The document object will continue to be the 1st argument to the function and a new 2nd argument will be the namespace of the source document. Fixes #55.\n\n\nFix an issue where a Date object created in Javascript would not be formatted correctly for indexing.\n\n\nBuild with go 1.10.1\n\n\n\n\nmonstache v4.3.1\n\n\n\n\nUpgrade gtm to pick up fix for parallel collection scans on direct reads. Each cursor now gets its own connection.\n\n\n\n\nmonstache v3.10.1\n\n\n\n\nUpgrade gtm to pick up fix for parallel collection scans on direct reads. Each cursor now gets its own connection.\n\n\n\n\nmonstache v4.3.0\n\n\n\n\nUpgrade gtm to pick up support for parallel collection scan on direct reads if your mongodb storage engine supports it\n\n\nAdd config option to specify the number of cursors to request for parallel collection scans\n\n\nAllow mappings to specify overrides for 1 of \nindex\n and \ntype\n instead of requiring both\n\n\nFix an issue where filters were not being applied to document updates\n\n\n\n\nmonstache v3.10.0\n\n\n\n\nUpgrade gtm to pick up support for parallel collection scan on direct reads if your mongodb storage engine supports it\n\n\nAdd config option to specify the number of cursors to request for parallel collection scans\n\n\nAllow mappings to specify overrides for 1 of \nindex\n and \ntype\n instead of requiring both\n\n\nFix an issue where filters were not being applied to document updates\n\n\n\n\nmonstache v4.2.1\n\n\n\n\nEnsure index names are lowercase \n\n\n\n\nmonstache v3.9.1\n\n\n\n\nEnsure index names are lowercase \n\n\n\n\nmonstache v4.2.0\n\n\n\n\nAdd filtering to Javascript and Golang plugins. Filtered documents are completely\n  ignored while dropped documents result in a delete request.\n\n\n\n\nmonstache v3.9.0\n\n\n\n\nAdd filtering to Javascript and Golang plugins. Filtered documents are completely\n  ignored while dropped documents result in a delete request.\n\n\n\n\nmonstache v4.1.2\n\n\n\n\nFix custom routing for golang plugins\n\n\nConfiguration now supports paths to Javascript files in addition to inline scripts\n\n\n\n\nmonstache v3.8.2\n\n\n\n\nFix custom routing for golang plugins\n\n\nConfiguration now supports paths to Javascript files in addition to inline scripts\n\n\n\n\nmonstache v4.1.1\n\n\n\n\nRoute time machine docs by MongoDB source id\n\n\n\n\nmonstache v3.8.1\n\n\n\n\nRoute time machine docs by MongoDB source id\n\n\n\n\nmonstache v4.1.0\n\n\n\n\nAdd a nifty time machine feature\n\n\n\n\nmonstache v3.8.0\n\n\n\n\nAdd a nifty time machine feature\n\n\n\n\nmonstache v4.0.1\n\n\n\n\nFixed a bug where monstache would think direct reads were done when they had not even started\n\n\nPerformance improvements for direct reads on large collections\n\n\n\n\nmonstache v3.7.0\n\n\n\n\nFixed a bug where monstache would think direct reads were done when they had not even started\n\n\nPerformance improvements for direct reads on large collections\n\n\n\n\nmonstache v4.0.0\n\n\n\n\n\n\nMonstache v4+ should be used for ES6+. There will still be bug fixes and maintenance done to the Monstache v3 releases to support ES2-5. You can still download v3.x releases from the downloads page or by directing go get to gopkg.in/rwynn/monstache.v3\n\n\n\n\n\n\nFixes deprecation warnings during bulk indexing against ES6 because of renamed fields version and version_type\n\n\n\n\n\n\nMonstache will now default to using the ES type _doc (as opposed to the MongoDB collection name) when it detects ES 6.2+. This is the new recommended type name going forward. See issue #42.\n\n\n\n\n\n\nmonstache v3.6.5\n\n\n\n\n\n\nRemove brittle normalization of index names, type names, and ids\n\n\n\n\n\n\nStart differentiating between releases supporting ES6+ and pre-ES6 by releasing from rel3 branch\n\n\n\n\n\n\nSoon a 4.0.0 release will be cut from master that will be ES6 forward. pre-ES6 will still be supported by downloading 3.x releases from the releases page or directing go get to gopkg.in/rwynn/monstache.v3\n\n\n\n\n\n\nTechnically this release will still work with ES+ but that won't last forever. There are some deprecation warnings. In summary, if you need pre-ES6 use v3.x releases of monstache and v4.x releases of monstache for ES6+ going forward.\n\n\n\n\n\n\nmonstache v3.6.4\n\n\n\n\n\n\nTrying to set the record for github releases in one night\n\n\n\n\n\n\nFix a regression whereby monstache would exit after direct reads were complete when it should have kept tailing the oplog\n\n\n\n\n\n\nmonstache v3.6.3\n\n\n\n\nFix for a benign race condition in shutdown, introduced in 3.6.2, that caused a panic\n\n\n\n\nmonstache v3.6.2\n\n\n\n\nResume usage of upstream elastic client library now that fix for Elasticsearch going down has been merged\n\n\nWhen Elasticsearch goes down the elastic client will now put back pressure on Add and Flush calls. When Elasticsearch comes back up it will resume Adding and Flushing were it left off. Do to the blocking nature of Add and Flush the shutdown function of monstache has been refactored to take this into account. Shutdown will not hang if Elasticsearch is down. It will try to Flush pending documents but if this blocks due to a down server it will still exit after a 5 second deadline.\n\n\n\n\nmonstache v3.6.1\n\n\n\n\nAdded more detailed error logging. Each bulk request line that failed will be logged separately with details. This is much more lightweight than having to turn on verbose to get error details. Verbose is not a recommended setting for production.\n\n\n\n\nmonstache v3.6.0\n\n\n\n\nThis release focuses on improvements with regards to handling dropped connections to either Elasticsearch or MongoDB and resuming gracefully when they come back online\n\n\n\n\nmonstache v3.5.2\n\n\n\n\nThe previous release safeguards the integrity of inserts and updates with a version number, but neglected deletes.  This release adds versions to deletes such that an [insert, delete] sequence that gets sent to Elasticsearch in 2 different requests (due to \nelasticsearch-max-conns\n \n 1) cannot actually perform a [delete, insert] instead.  In this case the insert would now carry a version number \n the delete version number and be rejected.    \n\n\n\n\nmonstache v3.5.1\n\n\n\n\nFix for issue #37 - out of order indexing due to concurrent bulk indexing requests.  With \nelasticsearch-max-conns\n set to greater than 1 you may get out of order index requests; however after this fix \neach document is versioned\n such that Elasticsearch will not replace a newer version with an older one.  The version of the document is the timestamp from the MongoDB oplog of when the change (insert, update) occurred.  Out of order indexing typically happens when both an insert and an update are queued for a bulk request at around the same time.  In this case, do to the way the bulk processor multiplexes requests onto multiple connections, the document may be received out of order. \n\n\n\n\nmonstache v3.5.0\n\n\n\n\nSupport for sharded MongoDB cluster.  See docs for details\n\n\nPerformance optimizations\n\n\nTurn off bulk retries if configured to do so\n\n\n\n\nmonstache v3.4.2\n\n\n\n\nAllow the stats index name format to be configurable.  Continues to default to index per day.  \n\n\n\n\nmonstache v3.4.1\n\n\n\n\nFix for the javascript mapping functions.  An Otto Export does not appear to recurse into arrays.  Need to do a recursive Export for this scenario.  \n\n\n\n\nmonstache v3.4.0\n\n\n\n\nAdd ability to embed documents during the mapping phase.  Javascript plugins get 3 new global functions: findId, findOne, and find.  Golang plugins get access to the mgo.Session.  See the docs for details.  \n\n\n\n\nmonstache v3.3.1\n\n\n\n\nImprove support for additional indexing metadata.\n\n\nFix issue where indexing metadata was not honored\n\n\n\n\nmonstache v3.3.0\n\n\n\n\nAdded optional http server.  Enable with --enable-http-server flag.  Listens on :8080 by default.  Configure address with --http-server-addr :8000.  The server responds to the following endpoints (/started, /healthz, /config, and /stats).  The stats endpoint is only enabled if stats are enabled. The /started and /healthz endpoints can be used to check for liveness.  \n\n\nUpgraded the gtm library with performance improvements\n\n\n\n\nmonstache v3.2.0\n\n\n\n\nAdd systemd support\n\n\n\n\nmonstache v3.1.2\n\n\n\n\nBuilt with go1.9\n\n\nFix golint warnings\n\n\n\n\nmonstache v3.1.1\n\n\n\n\ntimestamp stats indexes by day for easier cleanup using e.g. curator\n\n\n\n\nmonstache v3.1.0\n\n\n\n\nadd print-config argument to display the configuration and exit\n\n\nadd index-stats option to write indexing statistics into Elasticsearch for analysis\n\n\n\n\nmonstache v3.0.7\n\n\n\n\nfix elasticsearch client http scheme for secure connections\n\n\n\n\nmonstache v3.0.6\n\n\n\n\nfix invalid struct field tag\n\n\n\n\nmonstache v3.0.5\n\n\n\n\nadd direct-read-batch-size option\n\n\nupgrade gtm to accept batch size and to ensure all direct read errors are logged\n\n\n\n\nmonstache v3.0.4\n\n\n\n\nfix slowdown on direct reads for large mongodb collections\n\n\n\n\nmonstache v3.0.3\n\n\n\n\nsmall changes to the settings for the exponential back off on retry.  see the docs for details.\n\n\nonly record timestamps originating from the oplog and not from direct reads\n\n\napply the worker routing filter to direct reads in worker mode\n\n\n\n\nmonstache v3.0.2\n\n\n\n\nadd option to configure elasticsearch client http timout.  up the default timeout to 60 seconds\n\n\n\n\nmonstache v3.0.1\n\n\n\n\nupgrade gtm to fix an issue where a mongodb query error (such as CappedPositionLost) causes the tail go routine to exit (after which no more events will be processed)\n\n\n\n\nmonstache v3.0.0\n\n\n\n\nnew major release\n\n\nconfiguration changes with regards to Elasticsearch.  see docs for details\n\n\nadds ability to write rolling logs to files\n\n\nadds ability to log indexing statistics\n\n\nchanged go Elasticsearch client from elastigo to elastic which provides more API coverage\n\n\nupgrade gtm\n\n\n\n\nmonstache v2.14.0\n\n\n\n\nadd support for golang plugins.  you can now do in golang what you previously could do in javascript\n\n\nadd more detail to bulk indexing errors\n\n\nupgrade gtm\n\n\n\n\nmonstache v2.13.0\n\n\n\n\nadd direct-read-ns option.  allows one to sync documents directly from a set of collections in addition to going through the oplog\n\n\nadd exit-after-direct-reads option.  tells monstache to exit after performing direct reads.  useful for running monstache as a cron job.  \n\n\nfix issue around custom routing where db name was being stored as an array\n\n\nupgrade gtm\n\n\n\n\nmonstache v2.12.0\n\n\n\n\nFix order of operations surrounding db or collection drops in the oplog.  Required the removal of some gtm-options introduced in 2.11.  \n\n\nBuilt with latest version of gtm which includes some performance gains\n\n\nAdd ssl option under mongo-dial-settings.  Previously, in order to enable connections with TLS one had to provide a PEM file.  Now, one can enable TLS without a PEM file by setting this new option to true.  This was tested with MongoDB Atlas which requires SSL but does not provide a PEM file\n\n\n\n\nmonstache v2.11.2\n\n\n\n\nBuilt with Go 1.8\n\n\nAdded option \nfail-fast\n\n\nAdded option \nindex-oplog-time\n\n\n\n\nmonstache v2.11.1\n\n\n\n\nBuilt with Go 1.8\n\n\nPerformance improvements\n\n\nSupport for \nrfc7386\n JSON merge patches\n\n\nSupport for overriding Elasticsearch index and type in JavaScript\n\n\nMore configuration options surfaced\n\n\n\n\nmonstache v2.10.0\n\n\n\n\nadd shard routing capability\n\n\nadd Makefile\n\n\n\n\nmonstache v2.9.3\n\n\n\n\nextend ttl for active in cluster to reduce process switching\n\n\n\n\nmonstache v2.9.2\n\n\n\n\nfix potential collision on floating point _id closes #16\n\n\n\n\nmonstache v2.9.1\n\n\n\n\nfix an edge case #18 where a process resuming for the cluster would remain paused\n\n\n\n\nmonstache v2.9\n\n\n\n\nfix an issue with formatting of integer ids\n\n\nenable option for new clustering feature for high availability\n\n\nadd TLS skip verify options for mongodb and elasticsearch\n\n\nadd an option to specify a specific timestamp to start syncing from\n\n\n\n\nmonstache v2.8.1\n\n\n\n\nfix an index out of bounds panic during error reporting\n\n\nsurface gtm options for setting the oplog database and collection name as well as the cursor timeout\n\n\nreport an error if unable to unzip a response when verbose is true\n\n\n\n\nmonstache v2.8\n\n\n\n\nadd a version flag -v\n\n\ndocument the elasticsearch-pem-file option\n\n\nadd the elasticsearch-hosts option to configure pool of available nodes within a cluster\n\n\n\n\nmonstache v2.7\n\n\n\n\nadd a gzip configuration option to increase performance\n\n\ndefault resume-name to the worker name if defined\n\n\ndecrease binary size by building with -ldflags \"-w\"\n\n\n\n\nmonstache v2.6\n\n\n\n\nreuse allocations made for gridfs files\n\n\nadd workers feature to distribute synching between multiple processes\n\n\n\n\nmonstache v2.5\n\n\n\n\nadd option to speed up writes when saving resume state\n\n\nremove extra buffering when adding file content\n\n\n\n\nmonstache v2.4\n\n\n\n\nFixed issue #10\n\n\nFixed issue #11\n\n\n\n\nmonstache v2.3\n\n\n\n\nAdded configuration option for max file size\n\n\nAdded code to normalize index and type names based on restrictions in Elasticsearch\n\n\nPerformance improvements for GridFs files\n\n\n\n\nmonstache v2.2\n\n\n\n\nAdded configuration option for dropped databases and dropped collections.  See the README for more\n  information.\n\n\n\n\nmonstache v2.1\n\n\n\n\nAdded support for dropped databases and collections. Now when you drop a database or collection from\n  mongodb the corresponding indexes are deleted in elasticsearch.\n\n\n\n\nmonstache v2.0\n\n\n\n\nFixes an issue with the default mapping between mongodb and elasticsearch.  Previously, each database\n  in mongodb was mapped to an index of the same name in elasticsearch.  This creates a problem because\n  mongodb document ids are only guaranteed unique at the collection level.  If there are 2 or more documents\n  in a mongodb database with the same id those documents were previously written to the same elasticsearch index.\n  This fix changes the default mapping such that the entire mongodb document namespace (database + collection)\n  is mapped to the destination index in elasticsearch.  This prevents the possibility of collisions within\n  an index. Since this change requires reindexing of previously indexed data using monstache, the version \n  number of monstache was bumped to 2.  This change also means that by default you will have an index in\n  elasticsearch for each mongodb collection instead of each mongod database.  So more indexes by default.\n  You still have control to override the default mapping.  See the docs for how to explicitly control the index\n  and type used for a particular mongodb namespace.\n\n\nBumps the go version to 1.7.3 \n\n\n\n\nmonstache v1.3.1\n\n\n\n\nVersion 1.3 rebuilt with go1.7.1\n\n\n\n\nmonstache v1.3\n\n\n\n\nImprove log messages\n\n\nAdd support for the ingest-attachment plugin in elasticsearch 5\n\n\n\n\nmonstache v1.2\n\n\n\n\nImprove Error Reporting and Add Config Options\n\n\n\n\nmonstache v1.1\n\n\n\n\nFixes crash during replay (issue #2)\n\n\nAdds supports for indexing GridFS content (issue #3)\n\n\n\n\nmonstache v1.0\n\n\n\n\n64-bit Linux binary built with go1.6.2\n\n\n\n\nmonstache v0.8-beta.2\n\n\n\n\n64-bit Linux binary built with go1.6.2\n\n\n\n\nmonstache v0.8-beta.1\n\n\n\n\n64-bit Linux binary built with go1.6.2\n\n\n\n\nmonstache v0.8-beta\n\n\n\n\n64-bit Linux binary built with go1.6.2\n\n\n\n\nmonstache v0.8-alpha\n\n\n\n\n64-bit Linux binary built with go1.6.2", 
            "title": "About"
        }, 
        {
            "location": "/about/#about", 
            "text": "", 
            "title": "About"
        }, 
        {
            "location": "/about/#license", 
            "text": "The MIT License (MIT)  Copyright (c) 2016-2018 Ryan Wynn  Permission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.  THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.", 
            "title": "License"
        }, 
        {
            "location": "/about/#contributing", 
            "text": "The Monstache project welcomes, and depends, on contributions from developers and\nusers in the open source community. Contributions can be made in a number of\nways, a few examples are:   Code patches via pull requests  Documentation  improvements  Bug reports  and patch reviews", 
            "title": "Contributing"
        }, 
        {
            "location": "/about/#reporting-an-issue", 
            "text": "Please include as much detail as you can. Let us know your platform, Monstache\nversion, MongoDB version, and Elasticsearch version.", 
            "title": "Reporting an Issue"
        }, 
        {
            "location": "/about/#testing-the-development-version", 
            "text": "If you want to just install and try out the latest development version of\nMonstache you can do so with the following command. This can be useful if you\nwant to provide feedback for a new feature or want to confirm if a bug you\nhave encountered is fixed in the git master.  go get -u github.com/rwynn/monstache", 
            "title": "Testing the Development Version"
        }, 
        {
            "location": "/about/#running-the-tests", 
            "text": "To run the tests, you will need to have local mongod and elasticsearch servers running. \nThen you will need to start a monstache process in one terminal or in the background.  monstache -verbose  Finally in another terminal you can run the tests by issuing the following commands   cd $GOPATH/src/github.com/rwynn/monstache\ngo test -v   Warning  Running the Monstache tests will perform modifications to the  test.test  namespace in \nMongoDB and will index documents in the  test.test  index in Elasticsearch.  If you have\ndata that you need to keep on your local servers, make a back up before running the tests.", 
            "title": "Running the tests"
        }, 
        {
            "location": "/about/#submitting-pull-requests", 
            "text": "Once you are happy with your changes or you are ready for some feedback, push\nit to your fork and send a pull request. For a change to be accepted it will\nmost likely need to have tests and documentation if it is a new feature.", 
            "title": "Submitting Pull Requests"
        }, 
        {
            "location": "/about/#release-notes", 
            "text": "", 
            "title": "Release Notes"
        }, 
        {
            "location": "/about/#monstache-v463", 
            "text": "Fix for issue #65, year outside of [0,9999].  Invalid time will be removed now with  prune-invalid-json  turned on  Fix for issue #62, the number of connections to MongoDB is now limited to a max of 32 per namespace  Fix for issue #59, unsupported values of +/- Infinity and NaN.  These values can now be removed with the  prune-invalid-json  setting  Fix for issue $46 and #66, having to do with filtering.  Filters now use locks to ensure the javascript environment is used by one at a time.", 
            "title": "monstache v4.6.3"
        }, 
        {
            "location": "/about/#monstache-v3133", 
            "text": "Fix for issue #65, year outside of [0,9999].  Invalid time will be removed now with  prune-invalid-json  turned on  Fix for issue #62, the number of connections to MongoDB is now limited to a max of 32 per namespace  Fix for issue #59, unsupported values of +/- Infinity and NaN.  These values can now be removed with the  prune-invalid-json  setting  Fix for issue $46 and #66, having to do with filtering.  Filters now use locks to ensure the javascript environment is used by one at a time.", 
            "title": "monstache v3.13.3"
        }, 
        {
            "location": "/about/#monstache-v462", 
            "text": "Fix regression in 3.13 series where collections under 50K documents were not synching  Performing Tuning.  The following defaults have changed so please update your config files accordingly.   elasticsearch-max-conns went from 10 -  4\nelasticsearch-max-docs went from 1000 -  do not flush based on count (I suggest not overriding this since document sizes can vary greatly - instead use max-bytes)\nelasticsearch-max-bytes went from 5MB -  8MB  Note when you specify elasticseach-max-bytes the value must be in bytes not MB", 
            "title": "monstache v4.6.2"
        }, 
        {
            "location": "/about/#monstache-v3132", 
            "text": "Fix regression in 3.13 series where collections under 50K documents were not synching  Performing Tuning.  The following defaults have changed so please update your config files accordingly.   elasticsearch-max-conns went from 10 -  4\nelasticsearch-max-docs went from 1000 -  do not flush based on count (I suggest not overriding this since document sizes can vary greatly - instead use max-bytes)\nelasticsearch-max-bytes went from 5MB -  8MB  Note when you specify elasticseach-max-bytes the value must be in bytes not MB", 
            "title": "monstache v3.13.2"
        }, 
        {
            "location": "/about/#monstache-v461", 
            "text": "Performance and bug fixes in the gtm library", 
            "title": "monstache v4.6.1"
        }, 
        {
            "location": "/about/#monstache-v3131", 
            "text": "Performance and bug fixes in the gtm library", 
            "title": "monstache v3.13.1"
        }, 
        {
            "location": "/about/#monstache-v460", 
            "text": "Performance improvements. Much of the performance gains come from an upgrade of the gtm library. This library now uses split vector failing back to a paginated range queries. Also, some buffering has been removed at the gtm level for certain operations.", 
            "title": "monstache v4.6.0"
        }, 
        {
            "location": "/about/#monstache-v3130", 
            "text": "Performance improvements. Much of the performance gains come from an upgrade of the gtm library. This library now uses split vector failing back to a paginated range queries. Also, some buffering has been removed at the gtm level for certain operations.", 
            "title": "monstache v3.13.0"
        }, 
        {
            "location": "/about/#monstache-v450", 
            "text": "Adds an option  delete-index-pattern  to specify an Elasticsearch index pattern to scope stateless deletes.  Indexes outside of this\n  pattern will not be considered when propogating deletes from MongoDB to Elasticsearch.  By default all Elasticsearch indexes are queried.  Adds the ability to specify a global filter function in Javascript.  Previously, a filter function needed to be tied to a MongoDB namespace.\n  Now you can leave off the namespace and the filter function will be applied to all namespaces.  The filter function will receive the document\n  as the first argument and the MongoDB namespace as the second argument.  Breaking change:  direct-read-cursors  and  direct-read-batch-size  have been removed as options.  The underlying gtm library of monstache has been\n  upgraded and no longer supports parallelCollectionScan which is being removed in future versions of MongoDB.  Now gtm will use splitVector to divy\n  up collections to read documents concurrently. Also, the batch size will be managed by MongoDB and not set explicitly.  See the gtm library docs for more information.  Adds a boolean configuration option,  prune-invalid-json , which defaults to false.  Set this to true if your MongoDB data has values such as +Inf,\n  -Inf, or NaN which are not supported by the golang JSON parser and cause infinite error loops to occur.  With prune-invalid-json set to true Monstache\n  will remove these values before indexing into Elasticsearch to avoid these errors.", 
            "title": "monstache v4.5.0"
        }, 
        {
            "location": "/about/#monstache-v3120", 
            "text": "Adds an option  delete-index-pattern  to specify an Elasticsearch index pattern to scope stateless deletes.  Indexes outside of this\n  pattern will not be considered when propogating deletes from MongoDB to Elasticsearch.  By default all Elasticsearch indexes are queried.  Adds the ability to specify a global filter function in Javascript.  Previously, a filter function needed to be tied to a MongoDB namespace.\n  Now you can leave off the namespace and the filter function will be applied to all namespaces.  The filter function will receive the document\n  as the first argument and the MongoDB namespace as the second argument.  Breaking change:  direct-read-cursors  and  direct-read-batch-size  have been removed as options.  The underlying gtm library of monstache has been\n  upgraded and no longer supports parallelCollectionScan which is being removed in future versions of MongoDB.  Now gtm will use splitVector to divy\n  up collections to read documents concurrently. Also, the batch size will be managed by MongoDB and not set explicitly.  See the gtm library docs for more information.  Adds a boolean configuration option,  prune-invalid-json , which defaults to false.  Set this to true if your MongoDB data has values such as +Inf,\n  -Inf, or NaN which are not supported by the golang JSON parser and cause infinite error loops to occur.  With prune-invalid-json set to true Monstache\n  will remove these values before indexing into Elasticsearch to avoid these errors.", 
            "title": "monstache v3.12.0"
        }, 
        {
            "location": "/about/#monstache-v440", 
            "text": "Updated the default delete strategy    Breaking change: check  delete-strategy", 
            "title": "monstache v4.4.0"
        }, 
        {
            "location": "/about/#monstache-v3110", 
            "text": "Updated the default delete strategy    Breaking change: check  delete-strategy", 
            "title": "monstache v3.11.0"
        }, 
        {
            "location": "/about/#monstache-v432", 
            "text": "Allow specifying a script without a namespace. In this case documents from all collections will be run through the script. The document object will continue to be the 1st argument to the function and a new 2nd argument will be the namespace of the source document. Fixes #55.  Fix an issue where a Date object created in Javascript would not be formatted correctly for indexing.  Build with go 1.10.1", 
            "title": "monstache v4.3.2"
        }, 
        {
            "location": "/about/#monstache-v3102", 
            "text": "Allow specifying a script without a namespace. In this case documents from all collections will be run through the script. The document object will continue to be the 1st argument to the function and a new 2nd argument will be the namespace of the source document. Fixes #55.  Fix an issue where a Date object created in Javascript would not be formatted correctly for indexing.  Build with go 1.10.1", 
            "title": "monstache v3.10.2"
        }, 
        {
            "location": "/about/#monstache-v431", 
            "text": "Upgrade gtm to pick up fix for parallel collection scans on direct reads. Each cursor now gets its own connection.", 
            "title": "monstache v4.3.1"
        }, 
        {
            "location": "/about/#monstache-v3101", 
            "text": "Upgrade gtm to pick up fix for parallel collection scans on direct reads. Each cursor now gets its own connection.", 
            "title": "monstache v3.10.1"
        }, 
        {
            "location": "/about/#monstache-v430", 
            "text": "Upgrade gtm to pick up support for parallel collection scan on direct reads if your mongodb storage engine supports it  Add config option to specify the number of cursors to request for parallel collection scans  Allow mappings to specify overrides for 1 of  index  and  type  instead of requiring both  Fix an issue where filters were not being applied to document updates", 
            "title": "monstache v4.3.0"
        }, 
        {
            "location": "/about/#monstache-v3100", 
            "text": "Upgrade gtm to pick up support for parallel collection scan on direct reads if your mongodb storage engine supports it  Add config option to specify the number of cursors to request for parallel collection scans  Allow mappings to specify overrides for 1 of  index  and  type  instead of requiring both  Fix an issue where filters were not being applied to document updates", 
            "title": "monstache v3.10.0"
        }, 
        {
            "location": "/about/#monstache-v421", 
            "text": "Ensure index names are lowercase", 
            "title": "monstache v4.2.1"
        }, 
        {
            "location": "/about/#monstache-v391", 
            "text": "Ensure index names are lowercase", 
            "title": "monstache v3.9.1"
        }, 
        {
            "location": "/about/#monstache-v420", 
            "text": "Add filtering to Javascript and Golang plugins. Filtered documents are completely\n  ignored while dropped documents result in a delete request.", 
            "title": "monstache v4.2.0"
        }, 
        {
            "location": "/about/#monstache-v390", 
            "text": "Add filtering to Javascript and Golang plugins. Filtered documents are completely\n  ignored while dropped documents result in a delete request.", 
            "title": "monstache v3.9.0"
        }, 
        {
            "location": "/about/#monstache-v412", 
            "text": "Fix custom routing for golang plugins  Configuration now supports paths to Javascript files in addition to inline scripts", 
            "title": "monstache v4.1.2"
        }, 
        {
            "location": "/about/#monstache-v382", 
            "text": "Fix custom routing for golang plugins  Configuration now supports paths to Javascript files in addition to inline scripts", 
            "title": "monstache v3.8.2"
        }, 
        {
            "location": "/about/#monstache-v411", 
            "text": "Route time machine docs by MongoDB source id", 
            "title": "monstache v4.1.1"
        }, 
        {
            "location": "/about/#monstache-v381", 
            "text": "Route time machine docs by MongoDB source id", 
            "title": "monstache v3.8.1"
        }, 
        {
            "location": "/about/#monstache-v410", 
            "text": "Add a nifty time machine feature", 
            "title": "monstache v4.1.0"
        }, 
        {
            "location": "/about/#monstache-v380", 
            "text": "Add a nifty time machine feature", 
            "title": "monstache v3.8.0"
        }, 
        {
            "location": "/about/#monstache-v401", 
            "text": "Fixed a bug where monstache would think direct reads were done when they had not even started  Performance improvements for direct reads on large collections", 
            "title": "monstache v4.0.1"
        }, 
        {
            "location": "/about/#monstache-v370", 
            "text": "Fixed a bug where monstache would think direct reads were done when they had not even started  Performance improvements for direct reads on large collections", 
            "title": "monstache v3.7.0"
        }, 
        {
            "location": "/about/#monstache-v400", 
            "text": "Monstache v4+ should be used for ES6+. There will still be bug fixes and maintenance done to the Monstache v3 releases to support ES2-5. You can still download v3.x releases from the downloads page or by directing go get to gopkg.in/rwynn/monstache.v3    Fixes deprecation warnings during bulk indexing against ES6 because of renamed fields version and version_type    Monstache will now default to using the ES type _doc (as opposed to the MongoDB collection name) when it detects ES 6.2+. This is the new recommended type name going forward. See issue #42.", 
            "title": "monstache v4.0.0"
        }, 
        {
            "location": "/about/#monstache-v365", 
            "text": "Remove brittle normalization of index names, type names, and ids    Start differentiating between releases supporting ES6+ and pre-ES6 by releasing from rel3 branch    Soon a 4.0.0 release will be cut from master that will be ES6 forward. pre-ES6 will still be supported by downloading 3.x releases from the releases page or directing go get to gopkg.in/rwynn/monstache.v3    Technically this release will still work with ES+ but that won't last forever. There are some deprecation warnings. In summary, if you need pre-ES6 use v3.x releases of monstache and v4.x releases of monstache for ES6+ going forward.", 
            "title": "monstache v3.6.5"
        }, 
        {
            "location": "/about/#monstache-v364", 
            "text": "Trying to set the record for github releases in one night    Fix a regression whereby monstache would exit after direct reads were complete when it should have kept tailing the oplog", 
            "title": "monstache v3.6.4"
        }, 
        {
            "location": "/about/#monstache-v363", 
            "text": "Fix for a benign race condition in shutdown, introduced in 3.6.2, that caused a panic", 
            "title": "monstache v3.6.3"
        }, 
        {
            "location": "/about/#monstache-v362", 
            "text": "Resume usage of upstream elastic client library now that fix for Elasticsearch going down has been merged  When Elasticsearch goes down the elastic client will now put back pressure on Add and Flush calls. When Elasticsearch comes back up it will resume Adding and Flushing were it left off. Do to the blocking nature of Add and Flush the shutdown function of monstache has been refactored to take this into account. Shutdown will not hang if Elasticsearch is down. It will try to Flush pending documents but if this blocks due to a down server it will still exit after a 5 second deadline.", 
            "title": "monstache v3.6.2"
        }, 
        {
            "location": "/about/#monstache-v361", 
            "text": "Added more detailed error logging. Each bulk request line that failed will be logged separately with details. This is much more lightweight than having to turn on verbose to get error details. Verbose is not a recommended setting for production.", 
            "title": "monstache v3.6.1"
        }, 
        {
            "location": "/about/#monstache-v360", 
            "text": "This release focuses on improvements with regards to handling dropped connections to either Elasticsearch or MongoDB and resuming gracefully when they come back online", 
            "title": "monstache v3.6.0"
        }, 
        {
            "location": "/about/#monstache-v352", 
            "text": "The previous release safeguards the integrity of inserts and updates with a version number, but neglected deletes.  This release adds versions to deletes such that an [insert, delete] sequence that gets sent to Elasticsearch in 2 different requests (due to  elasticsearch-max-conns    1) cannot actually perform a [delete, insert] instead.  In this case the insert would now carry a version number   the delete version number and be rejected.", 
            "title": "monstache v3.5.2"
        }, 
        {
            "location": "/about/#monstache-v351", 
            "text": "Fix for issue #37 - out of order indexing due to concurrent bulk indexing requests.  With  elasticsearch-max-conns  set to greater than 1 you may get out of order index requests; however after this fix  each document is versioned  such that Elasticsearch will not replace a newer version with an older one.  The version of the document is the timestamp from the MongoDB oplog of when the change (insert, update) occurred.  Out of order indexing typically happens when both an insert and an update are queued for a bulk request at around the same time.  In this case, do to the way the bulk processor multiplexes requests onto multiple connections, the document may be received out of order.", 
            "title": "monstache v3.5.1"
        }, 
        {
            "location": "/about/#monstache-v350", 
            "text": "Support for sharded MongoDB cluster.  See docs for details  Performance optimizations  Turn off bulk retries if configured to do so", 
            "title": "monstache v3.5.0"
        }, 
        {
            "location": "/about/#monstache-v342", 
            "text": "Allow the stats index name format to be configurable.  Continues to default to index per day.", 
            "title": "monstache v3.4.2"
        }, 
        {
            "location": "/about/#monstache-v341", 
            "text": "Fix for the javascript mapping functions.  An Otto Export does not appear to recurse into arrays.  Need to do a recursive Export for this scenario.", 
            "title": "monstache v3.4.1"
        }, 
        {
            "location": "/about/#monstache-v340", 
            "text": "Add ability to embed documents during the mapping phase.  Javascript plugins get 3 new global functions: findId, findOne, and find.  Golang plugins get access to the mgo.Session.  See the docs for details.", 
            "title": "monstache v3.4.0"
        }, 
        {
            "location": "/about/#monstache-v331", 
            "text": "Improve support for additional indexing metadata.  Fix issue where indexing metadata was not honored", 
            "title": "monstache v3.3.1"
        }, 
        {
            "location": "/about/#monstache-v330", 
            "text": "Added optional http server.  Enable with --enable-http-server flag.  Listens on :8080 by default.  Configure address with --http-server-addr :8000.  The server responds to the following endpoints (/started, /healthz, /config, and /stats).  The stats endpoint is only enabled if stats are enabled. The /started and /healthz endpoints can be used to check for liveness.    Upgraded the gtm library with performance improvements", 
            "title": "monstache v3.3.0"
        }, 
        {
            "location": "/about/#monstache-v320", 
            "text": "Add systemd support", 
            "title": "monstache v3.2.0"
        }, 
        {
            "location": "/about/#monstache-v312", 
            "text": "Built with go1.9  Fix golint warnings", 
            "title": "monstache v3.1.2"
        }, 
        {
            "location": "/about/#monstache-v311", 
            "text": "timestamp stats indexes by day for easier cleanup using e.g. curator", 
            "title": "monstache v3.1.1"
        }, 
        {
            "location": "/about/#monstache-v310", 
            "text": "add print-config argument to display the configuration and exit  add index-stats option to write indexing statistics into Elasticsearch for analysis", 
            "title": "monstache v3.1.0"
        }, 
        {
            "location": "/about/#monstache-v307", 
            "text": "fix elasticsearch client http scheme for secure connections", 
            "title": "monstache v3.0.7"
        }, 
        {
            "location": "/about/#monstache-v306", 
            "text": "fix invalid struct field tag", 
            "title": "monstache v3.0.6"
        }, 
        {
            "location": "/about/#monstache-v305", 
            "text": "add direct-read-batch-size option  upgrade gtm to accept batch size and to ensure all direct read errors are logged", 
            "title": "monstache v3.0.5"
        }, 
        {
            "location": "/about/#monstache-v304", 
            "text": "fix slowdown on direct reads for large mongodb collections", 
            "title": "monstache v3.0.4"
        }, 
        {
            "location": "/about/#monstache-v303", 
            "text": "small changes to the settings for the exponential back off on retry.  see the docs for details.  only record timestamps originating from the oplog and not from direct reads  apply the worker routing filter to direct reads in worker mode", 
            "title": "monstache v3.0.3"
        }, 
        {
            "location": "/about/#monstache-v302", 
            "text": "add option to configure elasticsearch client http timout.  up the default timeout to 60 seconds", 
            "title": "monstache v3.0.2"
        }, 
        {
            "location": "/about/#monstache-v301", 
            "text": "upgrade gtm to fix an issue where a mongodb query error (such as CappedPositionLost) causes the tail go routine to exit (after which no more events will be processed)", 
            "title": "monstache v3.0.1"
        }, 
        {
            "location": "/about/#monstache-v300", 
            "text": "new major release  configuration changes with regards to Elasticsearch.  see docs for details  adds ability to write rolling logs to files  adds ability to log indexing statistics  changed go Elasticsearch client from elastigo to elastic which provides more API coverage  upgrade gtm", 
            "title": "monstache v3.0.0"
        }, 
        {
            "location": "/about/#monstache-v2140", 
            "text": "add support for golang plugins.  you can now do in golang what you previously could do in javascript  add more detail to bulk indexing errors  upgrade gtm", 
            "title": "monstache v2.14.0"
        }, 
        {
            "location": "/about/#monstache-v2130", 
            "text": "add direct-read-ns option.  allows one to sync documents directly from a set of collections in addition to going through the oplog  add exit-after-direct-reads option.  tells monstache to exit after performing direct reads.  useful for running monstache as a cron job.    fix issue around custom routing where db name was being stored as an array  upgrade gtm", 
            "title": "monstache v2.13.0"
        }, 
        {
            "location": "/about/#monstache-v2120", 
            "text": "Fix order of operations surrounding db or collection drops in the oplog.  Required the removal of some gtm-options introduced in 2.11.    Built with latest version of gtm which includes some performance gains  Add ssl option under mongo-dial-settings.  Previously, in order to enable connections with TLS one had to provide a PEM file.  Now, one can enable TLS without a PEM file by setting this new option to true.  This was tested with MongoDB Atlas which requires SSL but does not provide a PEM file", 
            "title": "monstache v2.12.0"
        }, 
        {
            "location": "/about/#monstache-v2112", 
            "text": "Built with Go 1.8  Added option  fail-fast  Added option  index-oplog-time", 
            "title": "monstache v2.11.2"
        }, 
        {
            "location": "/about/#monstache-v2111", 
            "text": "Built with Go 1.8  Performance improvements  Support for  rfc7386  JSON merge patches  Support for overriding Elasticsearch index and type in JavaScript  More configuration options surfaced", 
            "title": "monstache v2.11.1"
        }, 
        {
            "location": "/about/#monstache-v2100", 
            "text": "add shard routing capability  add Makefile", 
            "title": "monstache v2.10.0"
        }, 
        {
            "location": "/about/#monstache-v293", 
            "text": "extend ttl for active in cluster to reduce process switching", 
            "title": "monstache v2.9.3"
        }, 
        {
            "location": "/about/#monstache-v292", 
            "text": "fix potential collision on floating point _id closes #16", 
            "title": "monstache v2.9.2"
        }, 
        {
            "location": "/about/#monstache-v291", 
            "text": "fix an edge case #18 where a process resuming for the cluster would remain paused", 
            "title": "monstache v2.9.1"
        }, 
        {
            "location": "/about/#monstache-v29", 
            "text": "fix an issue with formatting of integer ids  enable option for new clustering feature for high availability  add TLS skip verify options for mongodb and elasticsearch  add an option to specify a specific timestamp to start syncing from", 
            "title": "monstache v2.9"
        }, 
        {
            "location": "/about/#monstache-v281", 
            "text": "fix an index out of bounds panic during error reporting  surface gtm options for setting the oplog database and collection name as well as the cursor timeout  report an error if unable to unzip a response when verbose is true", 
            "title": "monstache v2.8.1"
        }, 
        {
            "location": "/about/#monstache-v28", 
            "text": "add a version flag -v  document the elasticsearch-pem-file option  add the elasticsearch-hosts option to configure pool of available nodes within a cluster", 
            "title": "monstache v2.8"
        }, 
        {
            "location": "/about/#monstache-v27", 
            "text": "add a gzip configuration option to increase performance  default resume-name to the worker name if defined  decrease binary size by building with -ldflags \"-w\"", 
            "title": "monstache v2.7"
        }, 
        {
            "location": "/about/#monstache-v26", 
            "text": "reuse allocations made for gridfs files  add workers feature to distribute synching between multiple processes", 
            "title": "monstache v2.6"
        }, 
        {
            "location": "/about/#monstache-v25", 
            "text": "add option to speed up writes when saving resume state  remove extra buffering when adding file content", 
            "title": "monstache v2.5"
        }, 
        {
            "location": "/about/#monstache-v24", 
            "text": "Fixed issue #10  Fixed issue #11", 
            "title": "monstache v2.4"
        }, 
        {
            "location": "/about/#monstache-v23", 
            "text": "Added configuration option for max file size  Added code to normalize index and type names based on restrictions in Elasticsearch  Performance improvements for GridFs files", 
            "title": "monstache v2.3"
        }, 
        {
            "location": "/about/#monstache-v22", 
            "text": "Added configuration option for dropped databases and dropped collections.  See the README for more\n  information.", 
            "title": "monstache v2.2"
        }, 
        {
            "location": "/about/#monstache-v21", 
            "text": "Added support for dropped databases and collections. Now when you drop a database or collection from\n  mongodb the corresponding indexes are deleted in elasticsearch.", 
            "title": "monstache v2.1"
        }, 
        {
            "location": "/about/#monstache-v20", 
            "text": "Fixes an issue with the default mapping between mongodb and elasticsearch.  Previously, each database\n  in mongodb was mapped to an index of the same name in elasticsearch.  This creates a problem because\n  mongodb document ids are only guaranteed unique at the collection level.  If there are 2 or more documents\n  in a mongodb database with the same id those documents were previously written to the same elasticsearch index.\n  This fix changes the default mapping such that the entire mongodb document namespace (database + collection)\n  is mapped to the destination index in elasticsearch.  This prevents the possibility of collisions within\n  an index. Since this change requires reindexing of previously indexed data using monstache, the version \n  number of monstache was bumped to 2.  This change also means that by default you will have an index in\n  elasticsearch for each mongodb collection instead of each mongod database.  So more indexes by default.\n  You still have control to override the default mapping.  See the docs for how to explicitly control the index\n  and type used for a particular mongodb namespace.  Bumps the go version to 1.7.3", 
            "title": "monstache v2.0"
        }, 
        {
            "location": "/about/#monstache-v131", 
            "text": "Version 1.3 rebuilt with go1.7.1", 
            "title": "monstache v1.3.1"
        }, 
        {
            "location": "/about/#monstache-v13", 
            "text": "Improve log messages  Add support for the ingest-attachment plugin in elasticsearch 5", 
            "title": "monstache v1.3"
        }, 
        {
            "location": "/about/#monstache-v12", 
            "text": "Improve Error Reporting and Add Config Options", 
            "title": "monstache v1.2"
        }, 
        {
            "location": "/about/#monstache-v11", 
            "text": "Fixes crash during replay (issue #2)  Adds supports for indexing GridFS content (issue #3)", 
            "title": "monstache v1.1"
        }, 
        {
            "location": "/about/#monstache-v10", 
            "text": "64-bit Linux binary built with go1.6.2", 
            "title": "monstache v1.0"
        }, 
        {
            "location": "/about/#monstache-v08-beta2", 
            "text": "64-bit Linux binary built with go1.6.2", 
            "title": "monstache v0.8-beta.2"
        }, 
        {
            "location": "/about/#monstache-v08-beta1", 
            "text": "64-bit Linux binary built with go1.6.2", 
            "title": "monstache v0.8-beta.1"
        }, 
        {
            "location": "/about/#monstache-v08-beta", 
            "text": "64-bit Linux binary built with go1.6.2", 
            "title": "monstache v0.8-beta"
        }, 
        {
            "location": "/about/#monstache-v08-alpha", 
            "text": "64-bit Linux binary built with go1.6.2", 
            "title": "monstache v0.8-alpha"
        }
    ]
}